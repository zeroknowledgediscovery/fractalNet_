{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependencies found\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "\n",
    "path = '../cynet'\n",
    "sys.path.append(path)\n",
    "from cynet import cynet as cn\n",
    "import cynet_utils.spatial as sp\n",
    "D1=dir(cn)\n",
    "D2=dir(sp)\n",
    "d=[False,False,False,False]\n",
    "if 'Delaunay' in D1:\n",
    "    d[0]=True\n",
    "if 'LinearSegmentedColormap' in D2:\n",
    "    d[1]=True\n",
    "d[2]=True\n",
    "d[3]=True\n",
    "if not all(d):\n",
    "    raise Exception(\"Some dependencied are eithe rmissing or not in correct directories\")    \n",
    "else:\n",
    "    print('dependencies found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#path = '../cynet/'\n",
    "#sys.path.append(path)\n",
    "#import cynet.cynet as cn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Event Log File and Generate Split and Triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. File\n",
    "In the following cell, we specify the event log input file as `LOGFILE`. Since the `STOREFILE` is only used internally, we can just name it after the `LOGFILE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGFILE = 'https://zed.uchicago.edu/data/FN/terror.csv'\n",
    "STOREFILE = os.path.join('./', os.path.basename(LOGFILE).split('.')[0] + '.p')\n",
    "# In this case, STOREFILE = '/project2/ishanu/YI_terror/ntb/terror.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Spatial Range and Discretization\n",
    "\n",
    "In the following cell, we specify the tiles used for spatial discretization.\n",
    "We cut latitude (longitude) between `lat_min` and `lat_max` (`lon_min` and `lon_max`) into `lat_eps` (`lon_eps`) equal parts. Each tiles is one longitude step size wide and one latitude step size high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names in the event log file for coordinate 1 and 2\n",
    "coord1, coord2 ='latitude', 'longitude'\n",
    "\n",
    "# Tiles\n",
    "lat_min, lat_max = -4, 49\n",
    "lon_min, lon_max = -16, 84\n",
    "lat_eps, lon_eps = 4, 4\n",
    "\n",
    "lat = np.around(np.linspace(lat_min, lat_max, lat_eps + 1), decimals=5)\n",
    "lon = np.around(np.linspace(lon_min, lon_max, lon_eps + 1), decimals=5)\n",
    "tiles = [[lat[i], lat[i + 1], lon[j], lon[j + 1]] for i in np.arange(lat_eps) for j in np.arange(lon_eps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Time Range and Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names in the event log file for year, month, and day\n",
    "year, month, day='iyear', 'imonth', 'iday'\n",
    "init_date, end_date, freq = '2012-01-01', '2016-12-31', 'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Event\n",
    "If a time series has an event frequency less than `threshold`, discard the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\n",
    "    'number_of_kills': {\n",
    "        'col_name': 'nkill',\n",
    "        'value_limits': [0, 10000],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'NKILL'\n",
    "    },\n",
    "    'BEFIA': {\n",
    "        'col_name': 'attacktype1_txt',\n",
    "        'types': [[\n",
    "            'Bombing/Explosion', \n",
    "            'Facility/Infrastructure Attack'\n",
    "        ]],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'BEFIA'\n",
    "    },\n",
    "    'AAHHH': {\n",
    "        'col_name': 'attacktype1_txt',\n",
    "        'types': [[\n",
    "            'Armed Assault', \n",
    "            'Assassination',\n",
    "            'Hijacking',\n",
    "            'Hostage Taking (Barricade Incident)',\n",
    "            'Hostage Taking (Kidnapping)'\n",
    "        ]],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'AAHHH'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating Time Series for Training and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Time Series for the Number of kills\n",
    "Our first fit is `S0` for time series of number of kills. \n",
    "Essentially, we are looking for tiles that meet a certain number of kills (deaths in the column `nkill`). \n",
    "We are looking for tiles with number of kills that are greater than a certain `threshold`. \n",
    "Here that `threshold` is $0.025$.\n",
    "A file named `NKILL.csv` is outputted. \n",
    "And, more importantly, the internal timeseries dataframe is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:15<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "S0 = cn.spatioTemporal(\n",
    "    # File\n",
    "    log_file=LOGFILE,\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['number_of_kills']['col_name'],\n",
    "    value_limits=event_dict['number_of_kills']['value_limits'],\n",
    "    threshold=event_dict['number_of_kills']['threshold'])\n",
    "\n",
    "S0.fit(csvPREF=event_dict['number_of_kills']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we are now going to use the tiles selected for in `S0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = S0.getGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Time Series for Bombing/Explosion and Facility/Infrastructure Attack\n",
    "`S1` will be our fitting for attack types in the categories \n",
    " - `Bombing/Explosion` and \n",
    " - `Facility/Infrastructure Attack`.\n",
    "\n",
    "We are counting the number of these types of events that happen in these tiles.\n",
    "Output is written to `BEFIA.csv`, which contains the timeseries for those types of attacks in the selected tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:13<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "S1 = cn.spatioTemporal(\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['BEFIA']['col_name'],\n",
    "    types=event_dict['BEFIA']['types'],\n",
    "    threshold=event_dict['BEFIA']['threshold'])\n",
    "\n",
    "S1.fit(csvPREF=event_dict['BEFIA']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Time Series for Armed Assault, Assassination, Hijacking, and Hostage Taking\n",
    "`S2` fits for the attack types:\n",
    " - `Armed Assault`, \n",
    " - `Hostage Taking (Barricade Incident)`, \n",
    " - `Hijacking`, \n",
    " - `Assassination`,\n",
    " - `Hostage Taking (Kidnapping) `.\n",
    "\n",
    "Output is written to `AAHHH.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:13<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "S2 = cn.spatioTemporal(\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['AAHHH']['col_name'],\n",
    "    types=event_dict['AAHHH']['types'],\n",
    "    threshold=event_dict['AAHHH']['threshold'])\n",
    "\n",
    "S2.fit(csvPREF=event_dict['AAHHH']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Triplet for Training and Split for Testing\n",
    "Now we use the csv files created in previous steps (listed in `CSVfiles`) to generate the triplet files for training and split files for testing. \n",
    "\n",
    " - The triplet files are generated with `readTS`.\n",
    "    The training period is defined by `begin` and `end`. \n",
    " - The split files are generated with `splitTS`. \n",
    "    The split files contains data from `begin` to `extended_end`. \n",
    "    The data for testing are those beyond the `end` and before the `extended_end`\n",
    "    Here we set the `extended_end` to be one year beyond the `end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVfiles = [val['csvname_prefix'] + '.csv' for _, val in event_dict.items()]\n",
    "\n",
    "begin, end, extended_end = init_date, '2015-12-31', end_date\n",
    "\n",
    "# Make sure the triplet folder and split folder exist\n",
    "triplet_dir, split_dir = './triplet', './split'\n",
    "if not os.path.exists(triplet_dir):\n",
    "    os.makedirs(triplet_dir)\n",
    "if not os.path.exists(split_dir):\n",
    "    os.makedirs(split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet\n",
    "triplet_fnames_prefix = './triplet/TERROR_' + begin + '_' + end\n",
    "cn.readTS(\n",
    "    CSVfiles, \n",
    "    csvNAME=triplet_fnames_prefix, \n",
    "    BEG=begin, \n",
    "    END=end)\n",
    "\n",
    "# Split\n",
    "split_dirname = f'./split/'\n",
    "split_prefix = begin + '_' + extended_end + '_'\n",
    "cn.splitTS(\n",
    "    CSVfiles, \n",
    "    BEG=begin, \n",
    "    END=extended_end, \n",
    "    dirname=split_dirname, \n",
    "    prefix=split_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Optional cleanup of out-of-use files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CSVfile in CSVfiles:\n",
    "    os.remove(CSVfile)\n",
    "os.remove(STOREFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation\n",
    "Now that we training and testing data ready, it is time to create the models.\n",
    "\n",
    "**Input and Output of this step**\n",
    " - Input: training data (the triplet files) produced by `readTS`;\n",
    " - Output: model json files which each represnts a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting\n",
    "\n",
    "**Note:** It is highly recommended that we use absolute paths.\n",
    "\n",
    "**Explanations:**\n",
    " - `PARTITION`: Since we work with event counts, a single partitioning at $-.5$ makes \"no event\" a $0$, and \"any number of events more than $1$\" a $1$.\n",
    " - `RUN_LOCAL`: \n",
    "     - If `False`, `xgModels` will produce a list of calls `program_calls.txt` that needs to be run to produce the models.\n",
    "     - If `True`, `xgModels` will generate models locally. \n",
    " - `NUM_RERUNS`: Since `XgenESeSS` is random, we usually run it several times to get the averaged result.\n",
    " - `XgenESeSS`: The location of the `XgenESeSS` binary.\n",
    "     - it only work for Linux;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File parameters\n",
    "TS_PATH = triplet_fnames_prefix + '.csv' # The time series (data only)\n",
    "NAME_PATH = triplet_fnames_prefix + '.coords' # The names for each time series\n",
    "FILEPATH = './models/' # Make sure to create a folder with name `FILEPATH` below\n",
    "LOG_PATH = 'log.txt'\n",
    "\n",
    "# XgenESSeS parameters\n",
    "BEG = 1  # minimum delay considered\n",
    "END = 10 # maximum delay considered\n",
    "NUM_RERUNS = 2 # number of reruns\n",
    "PARTITION = [.5] # partitioning points. \n",
    "XgenESeSS = f'{path}/bin/XgenESeSS'\n",
    "RUN_LOCAL = True\n",
    "\n",
    "# make sure a folder named `models` is created\n",
    "if not os.path.exists(FILEPATH):\n",
    "    os.makedirs(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running `xgModels` to generate model or model generating calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XgenESeSS Command 1 has started\n",
      "XgenESeSS Command 2 has startedXgenESeSS Command 3 has started\n",
      "XgenESeSS Command 4 has startedXgenESeSS Command 5 has started\n",
      "XgenESeSS Command 6 has started\n",
      "XgenESeSS Command 7 has started\n",
      "\n",
      "\n",
      "XgenESeSS Command 8 has started\n",
      "XgenESeSS Command 9 has startedXgenESeSS Command 10 has started\n",
      "\n",
      "XgenESeSS Command 8 has finished\n",
      "XgenESeSS Command 11 has started\n",
      "XgenESeSS Command 10 has finished\n",
      "XgenESeSS Command 12 has started\n",
      "XgenESeSS Command 1 has finished\n",
      "XgenESeSS Command 13 has started\n",
      "XgenESeSS Command 11 has finished\n",
      "XgenESeSS Command 14 has started\n",
      "XgenESeSS Command 4 has finished\n",
      "XgenESeSS Command 15 has started\n",
      "XgenESeSS Command 6 has finished\n",
      "XgenESeSS Command 16 has started\n",
      "XgenESeSS Command 7 has finished\n",
      "XgenESeSS Command 17 has started\n",
      "XgenESeSS Command 3 has finished\n",
      "XgenESeSS Command 18 has started\n",
      "XgenESeSS Command 2 has finished\n",
      "XgenESeSS Command 19 has started\n",
      "XgenESeSS Command 12 has finished\n",
      "XgenESeSS Command 20 has started\n",
      "XgenESeSS Command 5 has finished\n",
      "XgenESeSS Command 21 has started\n",
      "XgenESeSS Command 9 has finished\n",
      "XgenESeSS Command 22 has started\n",
      "XgenESeSS Command 16 has finished\n",
      "XgenESeSS Command 23 has started\n",
      "XgenESeSS Command 13 has finished\n",
      "XgenESeSS Command 24 has started\n",
      "XgenESeSS Command 17 has finished\n",
      "XgenESeSS Command 25 has started\n",
      "XgenESeSS Command 14 has finished\n",
      "XgenESeSS Command 26 has started\n",
      "XgenESeSS Command 19 has finished\n",
      "XgenESeSS Command 27 has started\n",
      "XgenESeSS Command 18 has finished\n",
      "XgenESeSS Command 28 has started\n",
      "XgenESeSS Command 25 has finished\n",
      "XgenESeSS Command 29 has started\n",
      "XgenESeSS Command 15 has finished\n",
      "XgenESeSS Command 30 has started\n",
      "XgenESeSS Command 22 has finished\n",
      "XgenESeSS Command 31 has started\n",
      "XgenESeSS Command 20 has finished\n",
      "XgenESeSS Command 32 has started\n",
      "XgenESeSS Command 24 has finished\n",
      "XgenESeSS Command 33 has started\n",
      "XgenESeSS Command 26 has finished\n",
      "XgenESeSS Command 34 has started\n",
      "XgenESeSS Command 21 has finished\n",
      "XgenESeSS Command 35 has started\n",
      "XgenESeSS Command 27 has finished\n",
      "XgenESeSS Command 36 has started\n",
      "XgenESeSS Command 23 has finished\n",
      "XgenESeSS Command 37 has started\n",
      "XgenESeSS Command 30 has finished\n",
      "XgenESeSS Command 38 has started\n",
      "XgenESeSS Command 29 has finished\n",
      "XgenESeSS Command 39 has started\n",
      "XgenESeSS Command 33 has finished\n",
      "XgenESeSS Command 40 has started\n",
      "XgenESeSS Command 31 has finished\n",
      "XgenESeSS Command 41 has started\n",
      "XgenESeSS Command 28 has finished\n",
      "XgenESeSS Command 42 has started\n",
      "XgenESeSS Command 32 has finished\n",
      "XgenESeSS Command 43 has started\n",
      "XgenESeSS Command 35 has finished\n",
      "XgenESeSS Command 36 has finished\n",
      "XgenESeSS Command 34 has finished\n",
      "XgenESeSS Command 39 has finished\n",
      "XgenESeSS Command 40 has finished\n",
      "XgenESeSS Command 41 has finished\n",
      "XgenESeSS Command 38 has finished\n",
      "XgenESeSS Command 37 has finished\n",
      "XgenESeSS Command 43 has finished\n",
      "XgenESeSS Command 42 has finished\n",
      "Processing on XgenESeSS finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  43 out of  43 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "XG = cn.xgModels(\n",
    "    TS_PATH,\n",
    "    NAME_PATH, \n",
    "    LOG_PATH,\n",
    "    FILEPATH, \n",
    "    BEG, \n",
    "    END, \n",
    "    NUM_RERUNS, \n",
    "    PARTITION,\n",
    "    XgenESeSS,\n",
    "    RUN_LOCAL)\n",
    "XG.run(workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Here we evaluate our models by the AUC of the their prediction. \n",
    "\n",
    "The inner working of `run_pipeline`:\n",
    "1. It first select `model_nums` number of models either by gamma or distance. \n",
    "    Then it creates a model_sel json file which is a filtered version of the models.\n",
    "1. It applies the `cynet` binary to the model_sel files, which generates a log file containing predictions.\n",
    "1. It applies the `flexroc` binary to the log files, once for each target type.\n",
    "1. Finally, it writes test statistics (AUC, fpr, and, tpr) and output a `res_all.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting\n",
    "\n",
    "**Explanation:**\n",
    " - `RUNLEN`: number of time steps in training and testing;\n",
    " - `FLEX_TAIL_LEN`: number of time steps in testing;\n",
    " - `model_nums`: maximum number of models to use in prediction;\n",
    " - `horizon`: prediction horizon;\n",
    " - `VARNAME`: the predicting variable types;\n",
    "    Here we use individual variable types and `ALL` meaning all types of predicting variables are used together.\n",
    " - `gamma`: If `gamma` is true, the models are sorted with gamma (coefficient of causal dependence) and the best `model_nums` models will be used in the prediction;\n",
    " - To sort models by distance, use `distance=True` instead of `gamma=True` in `run_pipeline`;\n",
    " - `cores`: Number of cores running in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run length (train + test) = 1827\n",
      "test length = 366\n"
     ]
    }
   ],
   "source": [
    "# File parameters\n",
    "MODEL_GLOB = f'./models/*model.json'\n",
    "RESPATH = f'./models/*model*res'\n",
    "DATA_PATH = os.path.join(split_dirname, split_prefix) # the split files path prefix \n",
    "\n",
    "# Prediction parameters\n",
    "RUNLEN = len(pd.date_range(start=begin, end=extended_end, freq=freq))\n",
    "\n",
    "\n",
    "# Now we get the start of the test period.\n",
    "# Since the temporal resolution is 1 day, we just need to find the tomorrow of the training end.\n",
    "from datetime import datetime, timedelta\n",
    "start_of_test = datetime.strptime(end, '%Y-%m-%d') + timedelta(days=1)\n",
    "start_of_test = start_of_test.date()\n",
    "\n",
    "FLEX_TAIL_LEN = len(pd.date_range(start=start_of_test, end=extended_end, freq=freq))\n",
    "model_nums = [20]\n",
    "horizon = 7\n",
    "VARNAME = list(set([fname.split('#')[-1] for fname in glob(DATA_PATH + \"*\")])) + ['ALL']\n",
    "\n",
    "# Running parameters\n",
    "# Make sure you have multi-core access when using cores greater than 1. \n",
    "cores = 4\n",
    "\n",
    "print(f'run length (train + test) = {RUNLEN}\\ntest length = {FLEX_TAIL_LEN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  43 out of  43 | elapsed:   58.9s finished\n"
     ]
    }
   ],
   "source": [
    "cn.run_pipeline(\n",
    "    MODEL_GLOB,\n",
    "    model_nums, \n",
    "    horizon, \n",
    "    DATA_PATH, \n",
    "    RUNLEN, \n",
    "    VARNAME, \n",
    "    RESPATH, \n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN,\n",
    "    cores=cores,\n",
    "    gamma=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use see a summary of the aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    43.000000\n",
       "mean      0.818642\n",
       "std       0.088666\n",
       "min       0.500000\n",
       "25%       0.758695\n",
       "50%       0.810133\n",
       "75%       0.862928\n",
       "max       0.998403\n",
       "Name: auc, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.read_csv('res_all.csv')\n",
    "res[ (res['varsrc'] == 'ALL') & (res['auc'] < .999)]['auc'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Machine Learning Algorithms \n",
    "The cynet use a linear combination approach to aggregate the predictions from each source and form the final prediction for a target, but we can also apply machine learning algorithms in search for a better way to do the aggregation. \n",
    "For running machine learning algorithms, we need to run `cynet_chunker` to prepare csv files for running the machine leanring algorithms. \n",
    "\n",
    "### Parameter explanation\n",
    "Comparing to running cynet prediction, we only possible change we have to make is the `model_nums`.\n",
    "We can use a bigger number since machine learning algorithm can sometimes ignore noisy features\n",
    "The parameter `cores` is currently a dummy paramters. \n",
    "The `cynet_chunker` uses $1$ core not matter what values you enter. \n",
    "\n",
    "Make sure to create a folder named `csvs` before running `cynet_chunker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [25:55<00:00, 36.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# make sure a folder is created to contain the csv files used for machine learning algorithms\n",
    "if not os.path.exists('./csvs'):\n",
    "    os.makedirs('./csvs')\n",
    "\n",
    "model_nums_chunker = [200]\n",
    "cn.cynet_chunker(\n",
    "    MODEL_GLOB,\n",
    "    model_nums_chunker, \n",
    "    horizon, \n",
    "    DATA_PATH, \n",
    "    RUNLEN, \n",
    "    VARNAME, \n",
    "    RESPATH,\n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN,\n",
    "    cores=1, # dummy parameter\n",
    "    gamma=True,\n",
    "    PARTITION=PARTITION[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine leanring algorithm \n",
    "**Note:** Before Cynet is moved to Python3, we will have to run ML in a separate file.\n",
    "I will copy the code here, but please don't run.\n",
    "\n",
    "Currently, the code is in `MachineLearning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/43: model id = 19, auc = 0.764375\n",
      "1/43: model id = 16, auc = 0.713054\n",
      "2/43: model id = 27, auc = 0.949656\n",
      "3/43: model id = 23, auc = 0.737455\n",
      "4/43: model id = 1, auc = 0.77937\n",
      "5/43: model id = 5, auc = 0.819107\n",
      "6/43: model id = 9, auc = 0.5\n",
      "7/43: model id = 24, auc = 0.760469\n",
      "8/43: model id = 18, auc = 0.769654\n",
      "9/43: model id = 35, auc = 0.772486\n",
      "10/43: model id = 39, auc = 0.794572\n",
      "11/43: model id = 10, auc = 0.5\n",
      "12/43: model id = 0, auc = 0.793795\n",
      "13/43: model id = 32, auc = 0.759139\n",
      "14/43: model id = 6, auc = 0.799113\n",
      "15/43: model id = 17, auc = 0.848121\n",
      "16/43: model id = 25, auc = 0.688735\n",
      "17/43: model id = 8, auc = 0.86409\n",
      "18/43: model id = 14, auc = 0.827293\n",
      "19/43: model id = 41, auc = 0.836483\n",
      "20/43: model id = 13, auc = 0.977855\n",
      "21/43: model id = 2, auc = 0.881673\n",
      "22/43: model id = 36, auc = 0.821628\n",
      "23/43: model id = 33, auc = 0.803507\n",
      "24/43: model id = 37, auc = 0.886558\n",
      "25/43: model id = 34, auc = 0.801686\n",
      "26/43: model id = 22, auc = 0.846744\n",
      "27/43: model id = 15, auc = 0.622735\n",
      "28/43: model id = 31, auc = 0.820178\n",
      "29/43: model id = 30, auc = 0.834782\n",
      "30/43: model id = 12, auc = 0.781768\n",
      "31/43: model id = 3, auc = 0.691609\n",
      "32/43: model id = 42, auc = 0.79617\n",
      "33/43: model id = 21, auc = 0.811832\n",
      "34/43: model id = 11, auc = 0.773662\n",
      "35/43: model id = 7, auc = 0.642454\n",
      "36/43: model id = 29, auc = 0.810952\n",
      "37/43: model id = 38, auc = 0.989024\n",
      "38/43: model id = 26, auc = 0.750539\n",
      "39/43: model id = 20, auc = 0.80575\n",
      "40/43: model id = 40, auc = 0.698055\n",
      "41/43: model id = 28, auc = 0.814439\n",
      "42/43: model id = 4, auc = 0.819597\n",
      "            ml\n",
      "mean  0.785120\n",
      "50%   0.799113\n"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from glob import glob\n",
    "import subprocess\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import fbeta_score, make_scorer, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier, RandomForestRegressor, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# rom lightgbm import LGBMRegressor\n",
    "\n",
    "FLEXROC = os.path.dirname(sys.modules['cynet'].__file__) + '/bin/flexroc'\n",
    "\n",
    "# init_date, end_date, freq = '2012-01-01', '2016-12-31', 'D'\n",
    "# begin, end, extended_end = init_date, '2015-12-31', end_date\n",
    "# RUNLEN = len(pd.date_range(start=begin, end=extended_end, freq=freq))\n",
    "# FLEX_TAIL_LEN = len(pd.date_range(start=end, end=extended_end, freq=freq))\n",
    "# print(f'run length = {RUNLEN};\\nflex tail length = {FLEX_TAIL_LEN}.')\n",
    "TRAINLEN = RUNLEN - FLEX_TAIL_LEN\n",
    "\n",
    "def randomString(stringLength=8):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "\n",
    "\n",
    "def run_one_combined(filenames, partition, regr_name):\n",
    "\n",
    "    dfs = []\n",
    "    for i, filename in enumerate(filenames):\n",
    "        # Read in features csvs.\n",
    "        df = pd.read_csv(filename, header=None)\n",
    "\n",
    "        # Drop empty columns\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Renaming columns\n",
    "        cols = df.columns[1: -1]\n",
    "        cols = ['timestep'] + ['F_{}_{}'.format(i, x) for x in cols] + ['target']\n",
    "        df.columns = cols\n",
    "\n",
    "        # Drop out-of-sample rows.\n",
    "        df = df[df.target > -1]\n",
    "        \n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x > partition else 0)\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = reduce(lambda a, b: a.merge(b, how='inner', on=['timestep', 'target']), dfs)\n",
    "    df.set_index('timestep', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "    # remove identical columns and \n",
    "    # move target to the last column \n",
    "    cols_to_keep = []\n",
    "    for col in df:\n",
    "        a = df[col].unique()\n",
    "        if (col is not 'target') and (len(a) > 1):\n",
    "            cols_to_keep.append(col)\n",
    "    df = df[cols_to_keep + ['target']]\n",
    "\n",
    "\n",
    "    # Train test split\n",
    "    df_train = df[df.index < TRAINLEN]\n",
    "    df_test = df[df.index >= TRAINLEN]\n",
    "\n",
    "    data_train = df_train.values\n",
    "    data_test = df_test.values\n",
    "\n",
    "    X_train = data_train[:, :-1]\n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, :-1]\n",
    "    y_test = data_test[:, -1]\n",
    "\n",
    "\n",
    "    if np.count_nonzero(y_test) == 0:\n",
    "        return -1\n",
    "\n",
    "    # Define Regressor or Classifier.\n",
    "    TYPE_='REGR' #IXC\n",
    "    regrs = {\n",
    "        'ETree': ExtraTreesRegressor(\n",
    "            n_estimators=1000),\n",
    "        'ETree_cls': ExtraTreesClassifier(\n",
    "            n_estimators=500, \n",
    "            max_depth=None, \n",
    "            min_samples_split=2, \n",
    "            class_weight='balanced'),\n",
    "        'AdaB': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=None), n_estimators=100),\n",
    "        'NN': MLPClassifier(alpha=1, max_iter=1000)\n",
    "    }\n",
    "    regr = regrs[regr_name]\n",
    "    \n",
    "    # Train the regressor.\n",
    "    try: \n",
    "        regr.fit(X_train, y_train)\n",
    "        \n",
    "        if TYPE_=='REGR':\n",
    "            y_prob = regr.predict(X_test)\n",
    "            p_thresholds=y_prob\n",
    "        else:\n",
    "            y_prob = regr.predict_proba(X_test)\n",
    "            p_thresholds = [p[1] for p in y_prob]\n",
    "\n",
    "        # Very important, the y_test needs to be an int for flexroc to work properly.\n",
    "        to_write = [[int(y_test[n]), p_thresholds[n]] for n in range(len(p_thresholds))]\n",
    "\n",
    "        # Score or AUC\n",
    "        prefix = filename.rstrip('.csv')\n",
    "        log_filename = prefix + '.log'\n",
    "        roc_filename = prefix + '.roc'\n",
    "        with open(log_filename, 'w') as fh:\n",
    "            writer = csv.writer(fh, delimiter=' ')\n",
    "            for row in to_write:\n",
    "                writer.writerow(row)\n",
    "        auc_command = f'{FLEXROC} -i {log_filename}  -w 1 -x 0 -C 1 -L 1 -E 0 -f .2 -t .9 -r {roc_filename}'\n",
    "        output = subprocess.check_output(auc_command, shell=True)\n",
    "\n",
    "        # ------------add this line if working with python3 -----------------\n",
    "        output = output.decode('utf8')\n",
    "        # -------------------------------------------------------------------\n",
    "\n",
    "        auc = float(output.split(' ')[1])\n",
    "        return auc\n",
    "    except:\n",
    "        return -2\n",
    "\n",
    "\n",
    "csv_folder = './csvs'\n",
    "partition = .5\n",
    "regr_name = 'ETree'\n",
    "output_file = './ML_{regr_name}.csv'\n",
    "\n",
    "filenames = glob(f'{csv_folder}/*model.csv')\n",
    "model_ids, aucs = [], []\n",
    "for i, filename in enumerate(filenames):\n",
    "    model_id = filename.split('/')[-1][:-4].split('m')[0]\n",
    "    auc = run_one_combined([filename], partition, regr_name)\n",
    "    \n",
    "    model_ids.append(model_id)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "    print(f'{i}/{len(filenames)}: model id = {model_id}, auc = {auc}')\n",
    "\n",
    "df = pd.DataFrame(index=model_ids, data={'ml': aucs})\n",
    "df.index.name = 'model_id'\n",
    "df.sort_index(inplace=True)\n",
    "df.to_csv(output_file)\n",
    "\n",
    "df = df[df.ml > 0]\n",
    "print(df.describe().loc()[['mean', '50%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get simulation file\n",
    "In the next step, we will use the simulation file to get spatial relaxation and snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 344 out of 344 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concating 172 files.\n"
     ]
    }
   ],
   "source": [
    "log_path = FILEPATH\n",
    "\n",
    "cn.flexroc_only_parallel(\n",
    "    '{}/*.log'.format(log_path),\n",
    "    tpr_threshold=0.85,\n",
    "    fpr_threshold=None,\n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN, \n",
    "    cores=1)\n",
    "\n",
    "mapper = cn.mapped_events('{}/*{}models#*#*.csv'.format(log_path, model_nums[0]))\n",
    "mapper.concat_dataframes('{}/sim.csv'.format(log_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get spatial relaxation and plot snapshots\n",
    "\n",
    "The following code isn't really runable right now. But after we move cynet to Python3, it will be okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cynet_utils.spatial as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of days = 366\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../cynet/ntb/models/sim.csv does not exist: '../cynet/ntb/models/sim.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-649522b75a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatetime_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2016-01-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2016-12-31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'total number of days = {len(datetime_range)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/ntb/models/sim.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlat_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../cynet/ntb/models/sim.csv does not exist: '../cynet/ntb/models/sim.csv'"
     ]
    }
   ],
   "source": [
    "datetime_range = pd.date_range(start='2016-01-01', end='2016-12-31', freq='D')\n",
    "print(f'total number of days = {len(datetime_range)}')\n",
    "df = pd.read_csv('./models/sim.csv')\n",
    "\n",
    "lat_min, lat_max = df.lat1.min(), df.lat2.max()\n",
    "lon_min, lon_max = df.lon1.min(), df.lon2.max()\n",
    "print(f'lat = ({lat_min:.3f}, {lat_max:.3f})')\n",
    "print(f'lon = ({lon_min:.3f}, {lon_max:.3f})')\n",
    "\n",
    "day_min = 1462\n",
    "df = df[ (df['day'] >= day_min) & (df['target'] != 'VAR') ]\n",
    "\n",
    "\n",
    "print(df.target.unique())\n",
    "events = [\n",
    "    'Armed_Assault-Assassination-Hijacking-Hostage_Taking_Barricade_Incident-Hostage_Taking_Kidnapping',\n",
    "    'Bombing_Explosion-Facility_Infrastructure_Attack'\n",
    "]\n",
    "\n",
    "day = 1565\n",
    "time = datetime_range[day - day_min].date()\n",
    "print(time)\n",
    "\n",
    "\n",
    "Z = 0.06\n",
    "parameters = {e: {} for e in events}\n",
    "for var in events:\n",
    "    types = [var]\n",
    "    fn, tp, fp, ppv, sens, lon_, lat_, int_plot, int_, dfG, dfFN, dfTP, dfFP, dfTP0 = sp.get_prediction(\n",
    "        df,\n",
    "        day,\n",
    "        types,\n",
    "        lat_min,\n",
    "        lat_max,\n",
    "        lon_min,\n",
    "        lon_max,\n",
    "        radius=8,\n",
    "        sigma=3.5,\n",
    "        detail=1.2,\n",
    "        miles=15,\n",
    "        Z=Z)\n",
    "\n",
    "    print('day={}, {}: fp={}, tp={}, fn={}; ppv={:.3f}, sens={:.3f}'\n",
    "          .format(day, var, fp, tp, fn, ppv, sens))\n",
    "    \n",
    "    parameters[var].update({\n",
    "        'fp': fp, 'tp': tp, 'fn': fn,\n",
    "        'ppv': ppv, 'sens': sens,\n",
    "        'lon': lon_, 'lat': lat_,\n",
    "        'int': int_, 'df': dfG,\n",
    "        'dffp': dfFP, 'dftp': dfTP, 'dffn': dfFN,\n",
    "        'dftp_0': dfTP0\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap0 = sp.truncate_colormap(plt.cm.get_cmap('Reds'), 0.3, .9)\n",
    "cmap1 = sp.truncate_colormap(plt.cm.get_cmap('Blues'), 0.3, .9)\n",
    "bcolor0 = 'brown'\n",
    "bcolor1 = 'blue'\n",
    "\n",
    "\n",
    "parameters[events[0]]['cmap'] = cmap0\n",
    "parameters[events[0]]['bcolor'] = bcolor0\n",
    "parameters[events[0]]['mcolor'] = bcolor0\n",
    "parameters[events[0]]['mtype'] = 'v'\n",
    "parameters[events[1]]['cmap'] = cmap1\n",
    "parameters[events[1]]['bcolor'] = bcolor1\n",
    "parameters[events[1]]['mcolor'] = bcolor1\n",
    "parameters[events[1]]['mtype'] = '^'\n",
    "\n",
    "xlim = [-.1e7, 1.1e7]\n",
    "ylim =[-.12e7, .92e7]\n",
    "\n",
    "barTitleSize, barTickerSize = 20, 18\n",
    "barWidth, barHeight = 1.4, 1.76\n",
    "markerSize = 60\n",
    "lighten_ratio = .2\n",
    "\n",
    "width_height_ratio = (xlim[1] - xlim[0]) / (ylim[1] - ylim[0])\n",
    "height = 10\n",
    "width = width_height_ratio * height\n",
    "fig = plt.figure(figsize=(width, height))\n",
    "# A MUST-HAVE OR OTHERWISE THE DIMENSION OF THE PLOT ARE NOT UNIFORM\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(*xlim)\n",
    "ax.set_ylim(*ylim)\n",
    "\n",
    "var_title_map = {\n",
    "    events[0]: 'Personnel',\n",
    "    events[1]: 'Property'\n",
    "}\n",
    "for var in events:\n",
    "\n",
    "    fp, tp, fn = parameters[var]['fp'], parameters[var]['tp'], parameters[var]['fn']\n",
    "    lon_, lat_ = parameters[var]['lon'], parameters[var]['lat']\n",
    "    int_, dfG = parameters[var]['int'], parameters[var]['dftp']\n",
    "\n",
    "    dfG['Latitude'] = (dfG.lat1 + dfG.lat2) / 2\n",
    "    dfG['Longitude'] = (dfG.lon1 + dfG.lon2) / 2\n",
    "    dfG_ = dfG[['Latitude','Longitude']]\n",
    "    GND = gpd.GeoDataFrame(\n",
    "        dfG_, \n",
    "        geometry=gpd.points_from_xy(dfG_.Longitude, dfG_.Latitude))\n",
    "    GND.crs= {'init' :'epsg:4326'}\n",
    "    GND = GND.to_crs(epsg=3857)\n",
    "\n",
    "    Longitude = []\n",
    "    Latitude = []\n",
    "    density = []\n",
    "    for row in tqdm(np.arange(0,len(int_))):\n",
    "        for col in np.arange(0,len(int_[row])):\n",
    "            if int_[row, col] > 0:\n",
    "                Longitude = np.append(Longitude,lon_[row,col])\n",
    "                Latitude = np.append(Latitude,lat_[row,col])\n",
    "                density = np.append(density,int_[row,col])\n",
    "\n",
    "    df_density=pd.DataFrame(data={\n",
    "        'Longitude': Longitude, \n",
    "        'Latitude': Latitude, \n",
    "        'density':density})\n",
    "\n",
    "    wdf = gpd.GeoDataFrame(\n",
    "        df_density, \n",
    "        geometry=gpd.points_from_xy(df_density.Longitude, df_density.Latitude))\n",
    "    wdf.crs= {'init' :'epsg:4326'}\n",
    "    wdf = wdf.to_crs(epsg=3857)\n",
    "\n",
    "    ax = fig.gca()\n",
    "    ax = wdf.plot(\n",
    "        ax=ax,\n",
    "        column='density',\n",
    "        edgecolor='w',\n",
    "        linewidth=0,\n",
    "        cmap=parameters[var]['cmap'],\n",
    "        alpha=.6, \n",
    "        zorder=5, \n",
    "        marker='s',\n",
    "        markersize=135)\n",
    "\n",
    "    ax=GND.plot(\n",
    "        ax=ax, \n",
    "        marker=parameters[var]['mtype'],\n",
    "        lw=1,\n",
    "        # edgecolor='b',\n",
    "        facecolors=parameters[var]['mcolor'],\n",
    "        edgecolor=parameters[var]['mcolor'],\n",
    "        # color=lighten_color(parameters[var]['mcolor'], lighten_ratio), \n",
    "        markersize=150, \n",
    "        alpha=.8,\n",
    "        zorder=10)\n",
    "\n",
    "    ctx.add_basemap(ax, source=ctx.sources.ST_TONER_BACKGROUND,alpha=.5)  ###IXC\n",
    "\n",
    "\n",
    "# ======================== Time stamp ===================== START\n",
    "ax.text(\n",
    "    0.925, 0.05,\n",
    "    str(time).split()[0], \n",
    "    transform=ax.transAxes,\n",
    "    fontweight='bold',\n",
    "    fontsize=20,\n",
    "    color='w',\n",
    "    verticalalignment='bottom', \n",
    "    horizontalalignment='right', \n",
    "    bbox=dict(\n",
    "        boxstyle='round', \n",
    "        facecolor='midnightblue', \n",
    "        alpha=0.9)\n",
    ")\n",
    "# ======================== Time stamp ===================== END\n",
    "\n",
    "\n",
    "# ================== Add bars of FN, TP, FP ==============START\n",
    "width_ratio = barWidth / width\n",
    "height_ratio = barHeight / height\n",
    "parameters[events[0]]['bar_location'] = [0.625, 0.75, width_ratio, height_ratio]\n",
    "parameters[events[1]]['bar_location'] = [0.85, 0.75, width_ratio, height_ratio]\n",
    "\n",
    "plt.gcf().patches.extend([\n",
    "    plt.Rectangle(\n",
    "        (0.6, 0.7), \n",
    "        0.6, 0.25, \n",
    "        fill=True, \n",
    "        color='w', \n",
    "        alpha=0.9, \n",
    "        zorder=1,\n",
    "        transform=fig.transFigure, \n",
    "        figure=plt.gcf())\n",
    "])\n",
    "\n",
    "for var in events:\n",
    "    cmap = parameters[var]['cmap']\n",
    "    bar_location = parameters[var]['bar_location']\n",
    "    \n",
    "    bar_data = np.array([\n",
    "        parameters[var]['fn'], \n",
    "        parameters[var]['tp'], \n",
    "        parameters[var]['fp']\n",
    "    ])\n",
    "    bar_data = bar_data / bar_data.sum()\n",
    "\n",
    "    \n",
    "    ax2 = plt.gcf().add_axes(bar_location, zorder=20)\n",
    "    ax2.patch.set_alpha(1)\n",
    "    ax2.set_facecolor(\"white\")\n",
    "    \n",
    "    ax2.bar(\n",
    "        ['FN','TP','FP'],\n",
    "        bar_data,\n",
    "        color=parameters[var]['bcolor'],\n",
    "        lw=0,\n",
    "        zorder=20,\n",
    "        alpha=.9)\n",
    "\n",
    "    ax2.spines['bottom'].set_color('black')\n",
    "    ax2.spines['top'].set_color('black') \n",
    "    ax2.spines['right'].set_visible(False) \n",
    "    ax2.spines['left'].set_visible(False) \n",
    "    ax2.tick_params(axis='x', colors='black', pad=8)\n",
    "    ax2.tick_params(axis='y', colors='black')\n",
    "\n",
    "    ax2.set_title(\n",
    "        var_title_map[var], \n",
    "        color='black', \n",
    "        fontdict={'fontsize': barTitleSize, 'fontweight': 'bold'})\n",
    "    ttl = ax2.title\n",
    "    ttl.set_position([.5, 1.05])\n",
    "    ax2.grid(True)\n",
    "\n",
    "    for label in ax2.get_xticklabels():\n",
    "        label.set_color('black')\n",
    "        label.set_fontsize(barTickerSize)\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "    for label in ax2.get_yticklabels():\n",
    "        label.set_color('black')\n",
    "        label.set_fontsize(barTickerSize)\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "    ax2.tick_params(axis=u'both', which=u'both',length=0)\n",
    "# ================== Add bars of FN, TP, FP ==============END\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "sp.saveFIG(f'snapshot_{day}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
