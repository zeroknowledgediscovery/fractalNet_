{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '../'\n",
    "sys.path.append(path)\n",
    "import cynet.cynet as cn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Event Log File and Generate Split and Triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. File\n",
    "In the following cell, we specify the event log input file as `LOGFILE`. Since the `STOREFILE` is only used internally, we can just name it after the `LOGFILE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yihuang/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (4,6,31,33,53,61,62,63,76,79,90,92,94,96,114,115,121) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#LOGFILE = f'{path}/data/terror.csv'\n",
    "\n",
    "LOGFILE = 'https://zed.uchicago.edu/data/FN/terror.csv'\n",
    "pd.read_csv(LOGFILE)\n",
    "\n",
    "STOREFILE = os.path.join(f'{path}/ntb', os.path.basename(LOGFILE).split('.')[0] + '.p')\n",
    "# In this case, STOREFILE = '/project2/ishanu/YI_terror/ntb/terror.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Spatial Range and Discretization\n",
    "\n",
    "In the following cell, we specify the tiles used for spatial discretization.\n",
    "We cut latitude (longitude) between `lat_min` and `lat_max` (`lon_min` and `lon_max`) into `lat_eps` (`lon_eps`) equal parts. Each tiles is one longitude step size wide and one latitude step size high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names in the event log file for coordinate 1 and 2\n",
    "coord1, coord2 ='latitude', 'longitude'\n",
    "\n",
    "# Tiles\n",
    "lat_min, lat_max = -4, 49\n",
    "lon_min, lon_max = -16, 84\n",
    "lat_eps, lon_eps = 10, 10\n",
    "\n",
    "lat = np.around(np.linspace(lat_min, lat_max, lat_eps + 1), decimals=5)\n",
    "lon = np.around(np.linspace(lon_min, lon_max, lon_eps + 1), decimals=5)\n",
    "tiles = [[lat[i], lat[i + 1], lon[j], lon[j + 1]] for i in np.arange(lat_eps) for j in np.arange(lon_eps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Time Range and Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names in the event log file for year, month, and day\n",
    "year, month, day='iyear', 'imonth', 'iday'\n",
    "init_date, end_date, freq = '2012-01-01', '2016-12-31', 'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Event\n",
    "If a time series has an event frequency less than `threshold`, discard the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\n",
    "    'number_of_kills': {\n",
    "        'col_name': 'nkill',\n",
    "        'value_limits': [0, 10000],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'NKILL'\n",
    "    },\n",
    "    'BEFIA': {\n",
    "        'col_name': 'attacktype1_txt',\n",
    "        'types': [[\n",
    "            'Bombing/Explosion', \n",
    "            'Facility/Infrastructure Attack'\n",
    "        ]],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'BEFIA'\n",
    "    },\n",
    "    'AAHHH': {\n",
    "        'col_name': 'attacktype1_txt',\n",
    "        'types': [[\n",
    "            'Armed Assault', \n",
    "            'Assassination',\n",
    "            'Hijacking',\n",
    "            'Hostage Taking (Barricade Incident)',\n",
    "            'Hostage Taking (Kidnapping)'\n",
    "        ]],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'AAHHH'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating Time Series for Training and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Time Series for the Number of kills\n",
    "Our first fit is `S0` for time series of number of kills. \n",
    "Essentially, we are looking for tiles that meet a certain number of kills (deaths in the column `nkill`). \n",
    "We are looking for tiles with number of kills that are greater than a certain `threshold`. \n",
    "Here that `threshold` is $0.025$.\n",
    "A file named `NKILL.csv` is outputted. \n",
    "And, more importantly, the internal timeseries dataframe is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = cn.spatioTemporal(\n",
    "    # File\n",
    "    log_file=LOGFILE,\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['number_of_kills']['col_name'],\n",
    "    value_limits=event_dict['number_of_kills']['value_limits'],\n",
    "    threshold=event_dict['number_of_kills']['threshold'])\n",
    "\n",
    "S0.fit(csvPREF=event_dict['number_of_kills']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we are now going to use the tiles selected for in `S0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = S0.getGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Time Series for Bombing/Explosion and Facility/Infrastructure Attack\n",
    "`S1` will be our fitting for attack types in the categories \n",
    " - `Bombing/Explosion` and \n",
    " - `Facility/Infrastructure Attack`.\n",
    "\n",
    "We are counting the number of these types of events that happen in these tiles.\n",
    "Output is written to `BEFIA.csv`, which contains the timeseries for those types of attacks in the selected tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:30<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "S1 = cn.spatioTemporal(\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['BEFIA']['col_name'],\n",
    "    types=event_dict['BEFIA']['types'],\n",
    "    threshold=event_dict['BEFIA']['threshold'])\n",
    "\n",
    "S1.fit(csvPREF=event_dict['BEFIA']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Time Series for Armed Assault, Assassination, Hijacking, and Hostage Taking\n",
    "`S2` fits for the attack types:\n",
    " - `Armed Assault`, \n",
    " - `Hostage Taking (Barricade Incident)`, \n",
    " - `Hijacking`, \n",
    " - `Assassination`,\n",
    " - `Hostage Taking (Kidnapping) `.\n",
    "\n",
    "Output is written to `AAHHH.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:28<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "S2 = cn.spatioTemporal(\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['AAHHH']['col_name'],\n",
    "    types=event_dict['AAHHH']['types'],\n",
    "    threshold=event_dict['AAHHH']['threshold'])\n",
    "\n",
    "S2.fit(csvPREF=event_dict['AAHHH']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Triplet for Training and Split for Testing\n",
    "Now we use the csv files created in previous steps (listed in `CSVfiles`) to generate the triplet files for training and split files for testing. \n",
    "\n",
    " - The triplet files are generated with `readTS`.\n",
    "    The training period is defined by `begin` and `end`. \n",
    " - The split files are generated with `splitTS`. \n",
    "    The split files contains data from `begin` to `extended_end`. \n",
    "    The data for testing are those beyond the `end` and before the `extended_end`\n",
    "    Here we set the `extended_end` to be one year beyond the `end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVfiles = [val['csvname_prefix'] + '.csv' for _, val in event_dict.items()]\n",
    "\n",
    "begin, end, extended_end = init_date, '2015-12-31', end_date\n",
    "\n",
    "# Make sure the triplet folder and split folder exist\n",
    "triplet_dir, split_dir = f'{path}/ntb/triplet', f'{path}/ntb/split'\n",
    "if not os.path.exists(triplet_dir):\n",
    "    os.makedirs(triplet_dir)\n",
    "if not os.path.exists(split_dir):\n",
    "    os.makedirs(split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet\n",
    "triplet_fnames_prefix = f'{path}/ntb/triplet/TERROR_' + begin + '_' + end\n",
    "cn.readTS(\n",
    "    CSVfiles, \n",
    "    csvNAME=triplet_fnames_prefix, \n",
    "    BEG=begin, \n",
    "    END=end)\n",
    "\n",
    "# Split\n",
    "split_dirname = f'{path}/ntb/split/'\n",
    "split_prefix = begin + '_' + extended_end + '_'\n",
    "cn.splitTS(\n",
    "    CSVfiles, \n",
    "    BEG=begin, \n",
    "    END=extended_end, \n",
    "    dirname=split_dirname, \n",
    "    prefix=split_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Optional cleanup of out-of-use files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CSVfile in CSVfiles:\n",
    "    os.remove(CSVfile)\n",
    "os.remove(STOREFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation\n",
    "Now that we training and testing data ready, it is time to create the models.\n",
    "\n",
    "**Input and Output of this step**\n",
    " - Input: training data (the triplet files) produced by `readTS`;\n",
    " - Output: model json files which each represnts a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting\n",
    "\n",
    "**Note:** It is highly recommended that we use absolute paths.\n",
    "\n",
    "**Explanations:**\n",
    " - `PARTITION`: Since we work with event counts, a single partitioning at $-.5$ makes \"no event\" a $0$, and \"any number of events more than $1$\" a $1$.\n",
    " - `RUN_LOCAL`: \n",
    "     - If `False`, `xgModels` will produce a list of calls `program_calls.txt` that needs to be run to produce the models.\n",
    "     - If `True`, `xgModels` will generate models locally. \n",
    " - `NUM_RERUNS`: Since `XgenESeSS` is random, we usually run it several times to get the averaged result.\n",
    " - `XgenESeSS`: The location of the `XgenESeSS` binary.\n",
    "     - it only work for Linux;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File parameters\n",
    "TS_PATH = triplet_fnames_prefix + '.csv' # The time series (data only)\n",
    "NAME_PATH = triplet_fnames_prefix + '.coords' # The names for each time series\n",
    "FILEPATH = f'{path}/ntb/models/' # Make sure to create a folder with name `FILEPATH` below\n",
    "LOG_PATH = 'log.txt'\n",
    "\n",
    "# XgenESSeS parameters\n",
    "BEG = 1  # minimum delay considered\n",
    "END = 10 # maximum delay considered\n",
    "NUM_RERUNS = 2 # number of reruns\n",
    "PARTITION = [.5] # partitioning points. \n",
    "XgenESeSS = f'{path}/cynet/bin/XgenESeSS'\n",
    "RUN_LOCAL = True\n",
    "\n",
    "# make sure a folder named `models` is created\n",
    "if not os.path.exists(FILEPATH):\n",
    "    os.makedirs(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running `xgModels` to generate model or model generating calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XgenESeSS Command 1 has startedXgenESeSS Command 2 has started\n",
      "\n",
      "XgenESeSS Command 3 has started\n",
      "XgenESeSS Command 4 has startedXgenESeSS Command 5 has started\n",
      "XgenESeSS Command 6 has started\n",
      "XgenESeSS Command 7 has started\n",
      "\n",
      "XgenESeSS Command 8 has startedXgenESeSS Command 9 has startedXgenESeSS Command 10 has started\n",
      "\n",
      "\n",
      "XgenESeSS Command 5 has finished\n",
      "XgenESeSS Command 11 has started\n",
      "XgenESeSS Command 9 has finished\n",
      "XgenESeSS Command 12 has started\n",
      "XgenESeSS Command 3 has finished\n",
      "XgenESeSS Command 13 has started\n",
      "XgenESeSS Command 4 has finished\n",
      "XgenESeSS Command 14 has started\n",
      "XgenESeSS Command 10 has finished\n",
      "XgenESeSS Command 15 has started\n",
      "XgenESeSS Command 1 has finished\n",
      "XgenESeSS Command 16 has started\n",
      "XgenESeSS Command 2 has finished\n",
      "XgenESeSS Command 17 has started\n",
      "XgenESeSS Command 6 has finished\n",
      "XgenESeSS Command 18 has started\n",
      "XgenESeSS Command 8 has finished\n",
      "XgenESeSS Command 19 has started\n",
      "XgenESeSS Command 12 has finished\n",
      "XgenESeSS Command 20 has started\n",
      "XgenESeSS Command 11 has finished\n",
      "XgenESeSS Command 21 has started\n",
      "XgenESeSS Command 7 has finished\n",
      "XgenESeSS Command 22 has started\n",
      "XgenESeSS Command 15 has finished\n",
      "XgenESeSS Command 23 has started\n",
      "XgenESeSS Command 13 has finished\n",
      "XgenESeSS Command 24 has started\n",
      "XgenESeSS Command 16 has finished\n",
      "XgenESeSS Command 25 has started\n",
      "XgenESeSS Command 14 has finished\n",
      "XgenESeSS Command 26 has started\n",
      "XgenESeSS Command 21 has finished\n",
      "XgenESeSS Command 27 has started\n",
      "XgenESeSS Command 19 has finished\n",
      "XgenESeSS Command 28 has started\n",
      "XgenESeSS Command 17 has finished\n",
      "XgenESeSS Command 29 has started\n",
      "XgenESeSS Command 20 has finished\n",
      "XgenESeSS Command 30 has started\n",
      "XgenESeSS Command 23 has finished\n",
      "XgenESeSS Command 31 has started\n",
      "XgenESeSS Command 18 has finished\n",
      "XgenESeSS Command 32 has started\n",
      "XgenESeSS Command 26 has finished\n",
      "XgenESeSS Command 33 has started\n",
      "XgenESeSS Command 24 has finished\n",
      "XgenESeSS Command 34 has started\n",
      "XgenESeSS Command 22 has finished\n",
      "XgenESeSS Command 35 has started\n",
      "XgenESeSS Command 27 has finished\n",
      "XgenESeSS Command 36 has started\n",
      "XgenESeSS Command 25 has finished\n",
      "XgenESeSS Command 37 has started\n",
      "XgenESeSS Command 33 has finished\n",
      "XgenESeSS Command 38 has started\n",
      "XgenESeSS Command 29 has finished\n",
      "XgenESeSS Command 39 has started\n",
      "XgenESeSS Command 34 has finished\n",
      "XgenESeSS Command 40 has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed: 19.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XgenESeSS Command 35 has finished\n",
      "XgenESeSS Command 41 has started\n",
      "XgenESeSS Command 28 has finished\n",
      "XgenESeSS Command 42 has started\n",
      "XgenESeSS Command 30 has finished\n",
      "XgenESeSS Command 43 has started\n",
      "XgenESeSS Command 32 has finished\n",
      "XgenESeSS Command 44 has started\n",
      "XgenESeSS Command 36 has finished\n",
      "XgenESeSS Command 45 has started\n",
      "XgenESeSS Command 31 has finished\n",
      "XgenESeSS Command 46 has started\n",
      "XgenESeSS Command 40 has finished\n",
      "XgenESeSS Command 47 has started\n",
      "XgenESeSS Command 37 has finished\n",
      "XgenESeSS Command 48 has started\n",
      "XgenESeSS Command 42 has finished\n",
      "XgenESeSS Command 49 has started\n",
      "XgenESeSS Command 43 has finished\n",
      "XgenESeSS Command 50 has started\n",
      "XgenESeSS Command 38 has finished\n",
      "XgenESeSS Command 51 has started\n",
      "XgenESeSS Command 46 has finished\n",
      "XgenESeSS Command 52 has started\n",
      "XgenESeSS Command 44 has finished\n",
      "XgenESeSS Command 53 has started\n",
      "XgenESeSS Command 39 has finished\n",
      "XgenESeSS Command 54 has started\n",
      "XgenESeSS Command 41 has finished\n",
      "XgenESeSS Command 55 has started\n",
      "XgenESeSS Command 45 has finished\n",
      "XgenESeSS Command 56 has started\n",
      "XgenESeSS Command 50 has finished\n",
      "XgenESeSS Command 57 has started\n",
      "XgenESeSS Command 48 has finished\n",
      "XgenESeSS Command 58 has started\n",
      "XgenESeSS Command 49 has finished\n",
      "XgenESeSS Command 59 has started\n",
      "XgenESeSS Command 51 has finished\n",
      "XgenESeSS Command 60 has started\n",
      "XgenESeSS Command 54 has finished\n",
      "XgenESeSS Command 61 has started\n",
      "XgenESeSS Command 55 has finished\n",
      "XgenESeSS Command 62 has started\n",
      "XgenESeSS Command 56 has finished\n",
      "XgenESeSS Command 63 has started\n",
      "XgenESeSS Command 47 has finished\n",
      "XgenESeSS Command 64 has started\n",
      "XgenESeSS Command 52 has finished\n",
      "XgenESeSS Command 65 has started\n",
      "XgenESeSS Command 57 has finished\n",
      "XgenESeSS Command 66 has started\n",
      "XgenESeSS Command 58 has finished\n",
      "XgenESeSS Command 67 has started\n",
      "XgenESeSS Command 53 has finished\n",
      "XgenESeSS Command 68 has started\n",
      "XgenESeSS Command 60 has finished\n",
      "XgenESeSS Command 69 has started\n",
      "XgenESeSS Command 61 has finished\n",
      "XgenESeSS Command 70 has started\n",
      "XgenESeSS Command 63 has finished\n",
      "XgenESeSS Command 71 has started\n",
      "XgenESeSS Command 66 has finished\n",
      "XgenESeSS Command 72 has started\n",
      "XgenESeSS Command 62 has finished\n",
      "XgenESeSS Command 73 has started\n",
      "XgenESeSS Command 64 has finished\n",
      "XgenESeSS Command 74 has started\n",
      "XgenESeSS Command 67 has finished\n",
      "XgenESeSS Command 75 has started\n",
      "XgenESeSS Command 59 has finished\n",
      "XgenESeSS Command 76 has started\n",
      "XgenESeSS Command 65 has finished\n",
      "XgenESeSS Command 77 has started\n",
      "XgenESeSS Command 69 has finished\n",
      "XgenESeSS Command 78 has started\n",
      "XgenESeSS Command 68 has finished\n",
      "XgenESeSS Command 79 has started\n",
      "XgenESeSS Command 75 has finished\n",
      "XgenESeSS Command 80 has started\n",
      "XgenESeSS Command 74 has finished\n",
      "XgenESeSS Command 81 has started\n",
      "XgenESeSS Command 72 has finished\n",
      "XgenESeSS Command 82 has started\n",
      "XgenESeSS Command 70 has finished\n",
      "XgenESeSS Command 83 has started\n",
      "XgenESeSS Command 76 has finished\n",
      "XgenESeSS Command 84 has started\n",
      "XgenESeSS Command 71 has finished\n",
      "XgenESeSS Command 85 has started\n",
      "XgenESeSS Command 73 has finished\n",
      "XgenESeSS Command 86 has started\n",
      "XgenESeSS Command 82 has finished\n",
      "XgenESeSS Command 87 has started\n",
      "XgenESeSS Command 83 has finished\n",
      "XgenESeSS Command 88 has started\n",
      "XgenESeSS Command 81 has finished\n",
      "XgenESeSS Command 89 has started\n",
      "XgenESeSS Command 84 has finished\n",
      "XgenESeSS Command 90 has started\n",
      "XgenESeSS Command 79 has finished\n",
      "XgenESeSS Command 91 has started\n",
      "XgenESeSS Command 80 has finished\n",
      "XgenESeSS Command 92 has started\n",
      "XgenESeSS Command 77 has finished\n",
      "XgenESeSS Command 93 has started\n",
      "XgenESeSS Command 78 has finished\n",
      "XgenESeSS Command 94 has started\n",
      "XgenESeSS Command 86 has finished\n",
      "XgenESeSS Command 95 has started\n",
      "XgenESeSS Command 85 has finished\n",
      "XgenESeSS Command 96 has started\n",
      "XgenESeSS Command 91 has finished\n",
      "XgenESeSS Command 97 has started\n",
      "XgenESeSS Command 92 has finished\n",
      "XgenESeSS Command 98 has started\n",
      "XgenESeSS Command 90 has finished\n",
      "XgenESeSS Command 99 has started\n",
      "XgenESeSS Command 88 has finished\n",
      "XgenESeSS Command 100 has started\n",
      "XgenESeSS Command 89 has finished\n",
      "XgenESeSS Command 101 has started\n",
      "XgenESeSS Command 93 has finished\n",
      "XgenESeSS Command 102 has started\n",
      "XgenESeSS Command 87 has finished\n",
      "XgenESeSS Command 103 has started\n",
      "XgenESeSS Command 96 has finished\n",
      "XgenESeSS Command 104 has started\n",
      "XgenESeSS Command 98 has finished\n",
      "XgenESeSS Command 105 has started\n",
      "XgenESeSS Command 97 has finished\n",
      "XgenESeSS Command 106 has started\n",
      "XgenESeSS Command 99 has finished\n",
      "XgenESeSS Command 107 has started\n",
      "XgenESeSS Command 94 has finished\n",
      "XgenESeSS Command 108 has started\n",
      "XgenESeSS Command 95 has finished\n",
      "XgenESeSS Command 109 has started\n",
      "XgenESeSS Command 101 has finished\n",
      "XgenESeSS Command 110 has started\n",
      "XgenESeSS Command 100 has finished\n",
      "XgenESeSS Command 111 has started\n",
      "XgenESeSS Command 103 has finished\n",
      "XgenESeSS Command 112 has started\n",
      "XgenESeSS Command 102 has finished\n",
      "XgenESeSS Command 113 has started\n",
      "XgenESeSS Command 106 has finished\n",
      "XgenESeSS Command 114 has started\n",
      "XgenESeSS Command 107 has finished\n",
      "XgenESeSS Command 115 has started\n",
      "XgenESeSS Command 105 has finished\n",
      "XgenESeSS Command 116 has started\n",
      "XgenESeSS Command 104 has finished\n",
      "XgenESeSS Command 117 has started\n",
      "XgenESeSS Command 109 has finished\n",
      "XgenESeSS Command 118 has started\n",
      "XgenESeSS Command 113 has finished\n",
      "XgenESeSS Command 119 has started\n",
      "XgenESeSS Command 111 has finished\n",
      "XgenESeSS Command 120 has started\n",
      "XgenESeSS Command 114 has finished\n",
      "XgenESeSS Command 121 has started\n",
      "XgenESeSS Command 108 has finished\n",
      "XgenESeSS Command 122 has started\n",
      "XgenESeSS Command 110 has finished\n",
      "XgenESeSS Command 123 has started\n",
      "XgenESeSS Command 112 has finished\n",
      "XgenESeSS Command 124 has started\n",
      "XgenESeSS Command 117 has finished\n",
      "XgenESeSS Command 125 has started\n",
      "XgenESeSS Command 116 has finished\n",
      "XgenESeSS Command 126 has started\n",
      "XgenESeSS Command 123 has finished\n",
      "XgenESeSS Command 115 has finished\n",
      "XgenESeSS Command 124 has finished\n",
      "XgenESeSS Command 120 has finished\n",
      "XgenESeSS Command 122 has finished\n",
      "XgenESeSS Command 125 has finished\n",
      "XgenESeSS Command 121 has finished\n",
      "XgenESeSS Command 119 has finished\n",
      "XgenESeSS Command 118 has finished\n",
      "XgenESeSS Command 126 has finished\n",
      "Processing on XgenESeSS finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 126 out of 126 | elapsed: 65.1min finished\n"
     ]
    }
   ],
   "source": [
    "XG = cn.xgModels(\n",
    "    TS_PATH,\n",
    "    NAME_PATH, \n",
    "    LOG_PATH,\n",
    "    FILEPATH, \n",
    "    BEG, \n",
    "    END, \n",
    "    NUM_RERUNS, \n",
    "    PARTITION,\n",
    "    XgenESeSS,\n",
    "    RUN_LOCAL)\n",
    "XG.run(workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Here we evaluate our models by the AUC of the their prediction. \n",
    "\n",
    "The inner working of `run_pipeline`:\n",
    "1. It first select `model_nums` number of models either by gamma or distance. \n",
    "    Then it creates a model_sel json file which is a filtered version of the models.\n",
    "1. It applies the `cynet` binary to the model_sel files, which generates a log file containing predictions.\n",
    "1. It applies the `flexroc` binary to the log files, once for each target type.\n",
    "1. Finally, it writes test statistics (AUC, fpr, and, tpr) and output a `res_all.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting\n",
    "\n",
    "**Explanation:**\n",
    " - `RUNLEN`: number of time steps in training and testing;\n",
    " - `FLEX_TAIL_LEN`: number of time steps in testing;\n",
    " - `model_nums`: maximum number of models to use in prediction;\n",
    " - `horizon`: prediction horizon;\n",
    " - `VARNAME`: the predicting variable types;\n",
    "    Here we use individual variable types and `ALL` meaning all types of predicting variables are used together.\n",
    " - `gamma`: If `gamma` is true, the models are sorted with gamma (coefficient of causal dependence) and the best `model_nums` models will be used in the prediction;\n",
    " - To sort models by distance, use `distance=True` instead of `gamma=True` in `run_pipeline`;\n",
    " - `cores`: Number of cores running in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run length (train + test) = 1827\n",
      "test length = 366\n"
     ]
    }
   ],
   "source": [
    "# File parameters\n",
    "MODEL_GLOB = f'{path}/ntb/models/*model.json'\n",
    "RESPATH = f'{path}/ntb/models/*model*res'\n",
    "DATA_PATH = os.path.join(split_dirname, split_prefix) # the split files path prefix \n",
    "\n",
    "# Prediction parameters\n",
    "RUNLEN = len(pd.date_range(start=begin, end=extended_end, freq=freq))\n",
    "\n",
    "\n",
    "# Now we get the start of the test period.\n",
    "# Since the temporal resolution is 1 day, we just need to find the tomorrow of the training end.\n",
    "from datetime import datetime, timedelta\n",
    "start_of_test = datetime.strptime(end, '%Y-%m-%d') + timedelta(days=1)\n",
    "start_of_test = start_of_test.date()\n",
    "\n",
    "FLEX_TAIL_LEN = len(pd.date_range(start=start_of_test, end=extended_end, freq=freq))\n",
    "model_nums = [20]\n",
    "horizon = 7\n",
    "VARNAME = list(set([fname.split('#')[-1] for fname in glob(DATA_PATH + \"*\")])) + ['ALL']\n",
    "\n",
    "# Running parameters\n",
    "# Make sure you have multi-core access when using cores greater than 1. \n",
    "cores = 4\n",
    "\n",
    "print(f'run length (train + test) = {RUNLEN}\\ntest length = {FLEX_TAIL_LEN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   37.1s\n",
      "[Parallel(n_jobs=4)]: Done 126 out of 126 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "cn.run_pipeline(\n",
    "    MODEL_GLOB,\n",
    "    model_nums, \n",
    "    horizon, \n",
    "    DATA_PATH, \n",
    "    RUNLEN, \n",
    "    VARNAME, \n",
    "    RESPATH, \n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN,\n",
    "    cores=cores,\n",
    "    gamma=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use see a summary of the aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    126.000000\n",
       "mean       0.779978\n",
       "std        0.060360\n",
       "min        0.629792\n",
       "25%        0.745822\n",
       "50%        0.781466\n",
       "75%        0.812243\n",
       "max        0.963294\n",
       "Name: auc, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.read_csv('res_all.csv')\n",
    "res[ (res['varsrc'] == 'ALL') & (res['auc'] < .999)]['auc'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Machine Learning Algorithms \n",
    "The cynet use a linear combination approach to aggregate the predictions from each source and form the final prediction for a target, but we can also apply machine learning algorithms in search for a better way to do the aggregation. \n",
    "For running machine learning algorithms, we need to run `cynet_chunker` to prepare csv files for running the machine leanring algorithms. \n",
    "\n",
    "### Parameter explanation\n",
    "Comparing to running cynet prediction, we only possible change we have to make is the `model_nums`.\n",
    "We can use a bigger number since machine learning algorithm can sometimes ignore noisy features\n",
    "The parameter `cores` is currently a dummy paramters. \n",
    "The `cynet_chunker` uses $1$ core not matter what values you enter. \n",
    "\n",
    "Make sure to create a folder named `csvs` before running `cynet_chunker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [55:58<00:00, 26.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# make sure a folder is created to contain the csv files used for machine learning algorithms\n",
    "if not os.path.exists('csvs'):\n",
    "    os.makedirs('csvs')\n",
    "\n",
    "model_nums_chunker = [200]\n",
    "cn.cynet_chunker(\n",
    "    MODEL_GLOB,\n",
    "    model_nums_chunker, \n",
    "    horizon, \n",
    "    DATA_PATH, \n",
    "    RUNLEN, \n",
    "    VARNAME, \n",
    "    RESPATH,\n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN,\n",
    "    cores=1, # dummy parameter\n",
    "    gamma=True,\n",
    "    PARTITION=PARTITION[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine leanring algorithm \n",
    "**Note:** Before Cynet is moved to Python3, we will have to run ML in a separate file.\n",
    "I will copy the code here, but please don't run.\n",
    "\n",
    "Currently, the code is in `MachineLearning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/126: model id = 90, auc = 0.808433\n",
      "1/126: model id = 110, auc = 0.814852\n",
      "2/126: model id = 93, auc = 0.783105\n",
      "3/126: model id = 97, auc = 0.831118\n",
      "4/126: model id = 105, auc = 0.663275\n",
      "5/126: model id = 68, auc = 0.702031\n",
      "6/126: model id = 75, auc = 0.825557\n",
      "7/126: model id = 118, auc = 0.8276\n",
      "8/126: model id = 74, auc = 0.843898\n",
      "9/126: model id = 91, auc = 0.789524\n",
      "10/126: model id = 57, auc = 0.723338\n",
      "11/126: model id = 20, auc = 0.718081\n",
      "12/126: model id = 86, auc = 0.747396\n",
      "13/126: model id = 94, auc = 0.770939\n",
      "14/126: model id = 35, auc = 0.761604\n",
      "15/126: model id = 61, auc = 0.785656\n",
      "16/126: model id = 39, auc = 0.888332\n",
      "17/126: model id = 8, auc = 0.782781\n",
      "18/126: model id = 16, auc = 0.740422\n",
      "19/126: model id = 7, auc = 0.701985\n",
      "20/126: model id = 124, auc = 0.748417\n",
      "21/126: model id = 121, auc = 0.774399\n",
      "22/126: model id = 99, auc = 0.793347\n",
      "23/126: model id = 53, auc = 0.767048\n",
      "24/126: model id = 80, auc = 0.806484\n",
      "25/126: model id = 33, auc = 0.765984\n",
      "26/126: model id = 71, auc = 0.826322\n",
      "27/126: model id = 55, auc = 0.8125\n",
      "28/126: model id = 13, auc = 0.796855\n",
      "29/126: model id = 46, auc = 0.790848\n",
      "30/126: model id = 0, auc = 0.775308\n",
      "31/126: model id = 59, auc = 0.804343\n",
      "32/126: model id = 47, auc = 0.719581\n",
      "33/126: model id = 2, auc = 0.800765\n",
      "34/126: model id = 108, auc = 0.79161\n",
      "35/126: model id = 72, auc = 0.840758\n",
      "36/126: model id = 32, auc = 0.851876\n",
      "37/126: model id = 5, auc = 0.724705\n",
      "38/126: model id = 84, auc = 0.748871\n",
      "39/126: model id = 54, auc = 0.778937\n",
      "40/126: model id = 120, auc = 0.934944\n",
      "41/126: model id = 60, auc = 0.788286\n",
      "42/126: model id = 23, auc = 0.796588\n",
      "43/126: model id = 62, auc = 0.782763\n",
      "44/126: model id = 92, auc = 0.754981\n",
      "45/126: model id = 66, auc = 0.658911\n",
      "46/126: model id = 109, auc = 0.766557\n",
      "47/126: model id = 6, auc = 0.746856\n",
      "48/126: model id = 18, auc = 0.731695\n",
      "49/126: model id = 88, auc = 0.79359\n",
      "50/126: model id = 50, auc = 0.727799\n",
      "51/126: model id = 10, auc = 0.724168\n",
      "52/126: model id = 26, auc = 0.764456\n",
      "53/126: model id = 104, auc = 0.799188\n",
      "54/126: model id = 1, auc = 0.811568\n",
      "55/126: model id = 69, auc = 0.776291\n",
      "56/126: model id = 82, auc = 0.800594\n",
      "57/126: model id = 51, auc = 0.814703\n",
      "58/126: model id = 34, auc = 0.706403\n",
      "59/126: model id = 15, auc = 0.800155\n",
      "60/126: model id = 9, auc = 0.668698\n",
      "61/126: model id = 49, auc = 0.702097\n",
      "62/126: model id = 76, auc = 0.894426\n",
      "63/126: model id = 41, auc = 0.649155\n",
      "64/126: model id = 101, auc = 0.799458\n",
      "65/126: model id = 95, auc = 0.750917\n",
      "66/126: model id = 111, auc = 0.768851\n",
      "67/126: model id = 73, auc = 0.721067\n",
      "68/126: model id = 29, auc = 0.816768\n",
      "69/126: model id = 11, auc = 0.75839\n",
      "70/126: model id = 67, auc = 0.728247\n",
      "71/126: model id = 87, auc = 0.795338\n",
      "72/126: model id = 79, auc = 0.805473\n",
      "73/126: model id = 77, auc = 0.774234\n",
      "74/126: model id = 125, auc = 0.695307\n",
      "75/126: model id = 28, auc = 0.77649\n",
      "76/126: model id = 123, auc = 0.787414\n",
      "77/126: model id = 44, auc = 0.745293\n",
      "78/126: model id = 98, auc = 0.820393\n",
      "79/126: model id = 38, auc = 0.763135\n",
      "80/126: model id = 85, auc = 0.931193\n",
      "81/126: model id = 21, auc = 0.838938\n",
      "82/126: model id = 117, auc = 0.745335\n",
      "83/126: model id = 107, auc = 0.784435\n",
      "84/126: model id = 106, auc = 0.806975\n",
      "85/126: model id = 116, auc = 0.892307\n",
      "86/126: model id = 103, auc = 0.772635\n",
      "87/126: model id = 96, auc = 0.813796\n",
      "88/126: model id = 114, auc = 0.844627\n",
      "89/126: model id = 24, auc = 0.77804\n",
      "90/126: model id = 65, auc = 0.779575\n",
      "91/126: model id = 40, auc = 0.83671\n",
      "92/126: model id = 63, auc = 0.784697\n",
      "93/126: model id = 89, auc = 0.846637\n",
      "94/126: model id = 27, auc = 0.766671\n",
      "95/126: model id = 78, auc = 0.707527\n",
      "96/126: model id = 3, auc = 0.765472\n",
      "97/126: model id = 43, auc = 0.807085\n",
      "98/126: model id = 81, auc = 0.717347\n",
      "99/126: model id = 48, auc = 0.812852\n",
      "100/126: model id = 19, auc = 0.886378\n",
      "101/126: model id = 64, auc = 0.832589\n",
      "102/126: model id = 4, auc = 0.787961\n",
      "103/126: model id = 52, auc = 0.837471\n",
      "104/126: model id = 25, auc = 0.745042\n",
      "105/126: model id = 102, auc = 0.795964\n",
      "106/126: model id = 12, auc = 0.764237\n",
      "107/126: model id = 100, auc = 0.704645\n",
      "108/126: model id = 30, auc = 0.777047\n",
      "109/126: model id = 115, auc = 0.589579\n",
      "110/126: model id = 122, auc = 0.735576\n",
      "111/126: model id = 37, auc = 0.93442\n",
      "112/126: model id = 22, auc = 0.798745\n",
      "113/126: model id = 42, auc = 0.780315\n",
      "114/126: model id = 58, auc = 0.803891\n",
      "115/126: model id = 56, auc = 0.765903\n",
      "116/126: model id = 119, auc = 0.755963\n",
      "117/126: model id = 113, auc = 0.821042\n",
      "118/126: model id = 70, auc = 0.789576\n",
      "119/126: model id = 36, auc = 0.951719\n",
      "120/126: model id = 45, auc = 0.807042\n",
      "121/126: model id = 112, auc = 0.773226\n",
      "122/126: model id = 83, auc = 0.753415\n",
      "123/126: model id = 31, auc = 0.699437\n",
      "124/126: model id = 17, auc = 0.830634\n",
      "125/126: model id = 14, auc = 0.759224\n",
      "            ml\n",
      "mean  0.781305\n",
      "50%   0.782772\n"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from glob import glob\n",
    "import subprocess\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import fbeta_score, make_scorer, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier, RandomForestRegressor, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# rom lightgbm import LGBMRegressor\n",
    "\n",
    "FLEXROC = f'{path}/cynet/bin/flexroc'\n",
    "\n",
    "# init_date, end_date, freq = '2012-01-01', '2016-12-31', 'D'\n",
    "# begin, end, extended_end = init_date, '2015-12-31', end_date\n",
    "# RUNLEN = len(pd.date_range(start=begin, end=extended_end, freq=freq))\n",
    "# FLEX_TAIL_LEN = len(pd.date_range(start=end, end=extended_end, freq=freq))\n",
    "# print(f'run length = {RUNLEN};\\nflex tail length = {FLEX_TAIL_LEN}.')\n",
    "TRAINLEN = RUNLEN - FLEX_TAIL_LEN\n",
    "\n",
    "def randomString(stringLength=8):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "\n",
    "\n",
    "def run_one_combined(filenames, partition, regr_name):\n",
    "\n",
    "    dfs = []\n",
    "    for i, filename in enumerate(filenames):\n",
    "        # Read in features csvs.\n",
    "        df = pd.read_csv(filename, header=None)\n",
    "\n",
    "        # Drop empty columns\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Renaming columns\n",
    "        cols = df.columns[1: -1]\n",
    "        cols = ['timestep'] + ['F_{}_{}'.format(i, x) for x in cols] + ['target']\n",
    "        df.columns = cols\n",
    "\n",
    "        # Drop out-of-sample rows.\n",
    "        df = df[df.target > -1]\n",
    "        \n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x > partition else 0)\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = reduce(lambda a, b: a.merge(b, how='inner', on=['timestep', 'target']), dfs)\n",
    "    df.set_index('timestep', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "    # remove identical columns and \n",
    "    # move target to the last column \n",
    "    cols_to_keep = []\n",
    "    for col in df:\n",
    "        a = df[col].unique()\n",
    "        if (col is not 'target') and (len(a) > 1):\n",
    "            cols_to_keep.append(col)\n",
    "    df = df[cols_to_keep + ['target']]\n",
    "\n",
    "\n",
    "    # Train test split\n",
    "    df_train = df[df.index < TRAINLEN]\n",
    "    df_test = df[df.index >= TRAINLEN]\n",
    "\n",
    "    data_train = df_train.values\n",
    "    data_test = df_test.values\n",
    "\n",
    "    X_train = data_train[:, :-1]\n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, :-1]\n",
    "    y_test = data_test[:, -1]\n",
    "\n",
    "\n",
    "    if np.count_nonzero(y_test) == 0:\n",
    "        return -1\n",
    "\n",
    "    # Define Regressor or Classifier.\n",
    "    regrs = {\n",
    "        'ETree': ExtraTreesClassifier(\n",
    "            n_estimators=5000, \n",
    "            max_depth=2, \n",
    "            min_samples_split=2, \n",
    "            class_weight='balanced'),\n",
    "        'AdaB': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), n_estimators=1000),\n",
    "        'NN': MLPClassifier(alpha=1, max_iter=1000)\n",
    "    }\n",
    "    regr = regrs[regr_name]\n",
    "    \n",
    "    # Train the regressor.\n",
    "    try: \n",
    "        regr.fit(X_train, y_train)\n",
    "\n",
    "        y_prob = regr.predict_proba(X_test)\n",
    "        p_thresholds = [p[1] for p in y_prob]\n",
    "\n",
    "        # Very important, the y_test needs to be an int for flexroc to work properly.\n",
    "        to_write = [[int(y_test[n]), p_thresholds[n]] for n in range(len(p_thresholds))]\n",
    "\n",
    "        # Score or AUC\n",
    "        prefix = filename.rstrip('.csv')\n",
    "        log_filename = prefix + '.log'\n",
    "        roc_filename = prefix + '.roc'\n",
    "        with open(log_filename, 'w') as fh:\n",
    "            writer = csv.writer(fh, delimiter=' ')\n",
    "            for row in to_write:\n",
    "                writer.writerow(row)\n",
    "        auc_command = f'{FLEXROC} -i {log_filename}  -w 1 -x 0 -C 1 -L 1 -E 0 -f .2 -t .9 -r {roc_filename}'\n",
    "        output = subprocess.check_output(auc_command, shell=True)\n",
    "\n",
    "        # ------------add this line if working with python3 -----------------\n",
    "        output = output.decode('utf8')\n",
    "        # -------------------------------------------------------------------\n",
    "\n",
    "        auc = float(output.split(' ')[1])\n",
    "        return auc\n",
    "    except:\n",
    "        return -2\n",
    "\n",
    "\n",
    "csv_folder = f'{path}/ntb/csvs'\n",
    "partition = .5\n",
    "regr_name = 'ETree'\n",
    "output_file = f'{path}/ntb/ML_{regr_name}.csv'\n",
    "\n",
    "filenames = glob(f'{csv_folder}/*model.csv')\n",
    "model_ids, aucs = [], []\n",
    "for i, filename in enumerate(filenames):\n",
    "    model_id = filename.split('/')[-1][:-4].split('m')[0]\n",
    "    auc = run_one_combined([filename], partition, regr_name)\n",
    "    \n",
    "    model_ids.append(model_id)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "    print(f'{i}/{len(filenames)}: model id = {model_id}, auc = {auc}')\n",
    "\n",
    "df = pd.DataFrame(index=model_ids, data={'ml': aucs})\n",
    "df.index.name = 'model_id'\n",
    "df.sort_index(inplace=True)\n",
    "df.to_csv(output_file)\n",
    "\n",
    "df = df[df.ml > 0]\n",
    "print(df.describe().loc()[['mean', '50%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get simulation file\n",
    "In the next step, we will use the simulation file to get spatial relaxation and snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1008 out of 1008 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concating 504 files.\n"
     ]
    }
   ],
   "source": [
    "log_path = FILEPATH\n",
    "\n",
    "cn.flexroc_only_parallel(\n",
    "    '{}/*.log'.format(log_path),\n",
    "    tpr_threshold=0.85,\n",
    "    fpr_threshold=None,\n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN, \n",
    "    cores=1)\n",
    "\n",
    "mapper = cn.mapped_events('{}/*{}models#*#*.csv'.format(log_path, model_nums[0]))\n",
    "mapper.concat_dataframes('{}/sim.csv'.format(log_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get spatial relaxation and plot snapshots\n",
    "\n",
    "The following code isn't really runable right now. But after we move cynet to Python3, it will be okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'{path}/cynet_utils/')\n",
    "import spatial as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of days = 366\n",
      "lat = (-4.000, 49.000)\n",
      "lon = (-6.000, 84.000)\n",
      "['Armed_Assault-Assassination-Hijacking-Hostage_Taking_Barricade_Incident-Hostage_Taking_Kidnapping'\n",
      " 'Bombing_Explosion-Facility_Infrastructure_Attack']\n",
      "2016-04-13\n",
      "tmporal comp: -->  tp  29  fp  46  fn  18\n",
      "day=1565, Armed_Assault-Assassination-Hijacking-Hostage_Taking_Barricade_Incident-Hostage_Taking_Kidnapping: fp=0, tp=29, fn=6; ppv=1.000, sens=0.829\n",
      "tmporal comp: -->  tp  25  fp  51  fn  11\n",
      "day=1565, Bombing_Explosion-Facility_Infrastructure_Attack: fp=0, tp=25, fn=4; ppv=1.000, sens=0.862\n"
     ]
    }
   ],
   "source": [
    "datetime_range = pd.date_range(start='2016-01-01', end='2016-12-31', freq='D')\n",
    "print(f'total number of days = {len(datetime_range)}')\n",
    "df = pd.read_csv(f'{path}/ntb/models/sim.csv')\n",
    "\n",
    "lat_min, lat_max = df.lat1.min(), df.lat2.max()\n",
    "lon_min, lon_max = df.lon1.min(), df.lon2.max()\n",
    "print(f'lat = ({lat_min:.3f}, {lat_max:.3f})')\n",
    "print(f'lon = ({lon_min:.3f}, {lon_max:.3f})')\n",
    "\n",
    "day_min = 1462\n",
    "df = df[ (df['day'] >= day_min) & (df['target'] != 'VAR') ]\n",
    "\n",
    "\n",
    "print(df.target.unique())\n",
    "events = [\n",
    "    'Armed_Assault-Assassination-Hijacking-Hostage_Taking_Barricade_Incident-Hostage_Taking_Kidnapping',\n",
    "    'Bombing_Explosion-Facility_Infrastructure_Attack'\n",
    "]\n",
    "\n",
    "day = 1565\n",
    "time = datetime_range[day - day_min].date()\n",
    "print(time)\n",
    "\n",
    "\n",
    "Z = 0.06\n",
    "parameters = {e: {} for e in events}\n",
    "for var in events:\n",
    "    types = [var]\n",
    "    fn, tp, fp, ppv, sens, lon_, lat_, int_plot, int_, dfG, dfFN, dfTP, dfFP, dfTP0 = sp.get_prediction(\n",
    "        df,\n",
    "        day,\n",
    "        types,\n",
    "        lat_min,\n",
    "        lat_max,\n",
    "        lon_min,\n",
    "        lon_max,\n",
    "        radius=8,\n",
    "        sigma=3.5,\n",
    "        detail=1.2,\n",
    "        miles=15,\n",
    "        Z=Z)\n",
    "\n",
    "    print('day={}, {}: fp={}, tp={}, fn={}; ppv={:.3f}, sens={:.3f}'\n",
    "          .format(day, var, fp, tp, fn, ppv, sens))\n",
    "    \n",
    "    parameters[var].update({\n",
    "        'fp': fp, 'tp': tp, 'fn': fn,\n",
    "        'ppv': ppv, 'sens': sens,\n",
    "        'lon': lon_, 'lat': lat_,\n",
    "        'int': int_, 'df': dfG,\n",
    "        'dffp': dfFP, 'dftp': dfTP, 'dffn': dfFN,\n",
    "        'dftp_0': dfTP0\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:00<00:00, 6640.36it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 8962.52it/s]\n"
     ]
    }
   ],
   "source": [
    "cmap0 = sp.truncate_colormap(plt.cm.get_cmap('Reds'), 0.3, .9)\n",
    "cmap1 = sp.truncate_colormap(plt.cm.get_cmap('Blues'), 0.3, .9)\n",
    "bcolor0 = 'brown'\n",
    "bcolor1 = 'blue'\n",
    "\n",
    "\n",
    "parameters[events[0]]['cmap'] = cmap0\n",
    "parameters[events[0]]['bcolor'] = bcolor0\n",
    "parameters[events[0]]['mcolor'] = bcolor0\n",
    "parameters[events[0]]['mtype'] = 'v'\n",
    "parameters[events[1]]['cmap'] = cmap1\n",
    "parameters[events[1]]['bcolor'] = bcolor1\n",
    "parameters[events[1]]['mcolor'] = bcolor1\n",
    "parameters[events[1]]['mtype'] = '^'\n",
    "\n",
    "xlim = [-.1e7, 1.1e7]\n",
    "ylim =[-.12e7, .92e7]\n",
    "\n",
    "barTitleSize, barTickerSize = 20, 18\n",
    "barWidth, barHeight = 1.4, 1.76\n",
    "markerSize = 60\n",
    "lighten_ratio = .2\n",
    "\n",
    "width_height_ratio = (xlim[1] - xlim[0]) / (ylim[1] - ylim[0])\n",
    "height = 10\n",
    "width = width_height_ratio * height\n",
    "fig = plt.figure(figsize=(width, height))\n",
    "# A MUST-HAVE OR OTHERWISE THE DIMENSION OF THE PLOT ARE NOT UNIFORM\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(*xlim)\n",
    "ax.set_ylim(*ylim)\n",
    "\n",
    "var_title_map = {\n",
    "    events[0]: 'Personnel',\n",
    "    events[1]: 'Property'\n",
    "}\n",
    "for var in events:\n",
    "\n",
    "    fp, tp, fn = parameters[var]['fp'], parameters[var]['tp'], parameters[var]['fn']\n",
    "    lon_, lat_ = parameters[var]['lon'], parameters[var]['lat']\n",
    "    int_, dfG = parameters[var]['int'], parameters[var]['dftp']\n",
    "\n",
    "    dfG['Latitude'] = (dfG.lat1 + dfG.lat2) / 2\n",
    "    dfG['Longitude'] = (dfG.lon1 + dfG.lon2) / 2\n",
    "    dfG_ = dfG[['Latitude','Longitude']]\n",
    "    GND = gpd.GeoDataFrame(\n",
    "        dfG_, \n",
    "        geometry=gpd.points_from_xy(dfG_.Longitude, dfG_.Latitude))\n",
    "    GND.crs= {'init' :'epsg:4326'}\n",
    "    GND = GND.to_crs(epsg=3857)\n",
    "\n",
    "    Longitude = []\n",
    "    Latitude = []\n",
    "    density = []\n",
    "    for row in tqdm(np.arange(0,len(int_))):\n",
    "        for col in np.arange(0,len(int_[row])):\n",
    "            if int_[row, col] > 0:\n",
    "                Longitude = np.append(Longitude,lon_[row,col])\n",
    "                Latitude = np.append(Latitude,lat_[row,col])\n",
    "                density = np.append(density,int_[row,col])\n",
    "\n",
    "    df_density=pd.DataFrame(data={\n",
    "        'Longitude': Longitude, \n",
    "        'Latitude': Latitude, \n",
    "        'density':density})\n",
    "\n",
    "    wdf = gpd.GeoDataFrame(\n",
    "        df_density, \n",
    "        geometry=gpd.points_from_xy(df_density.Longitude, df_density.Latitude))\n",
    "    wdf.crs= {'init' :'epsg:4326'}\n",
    "    wdf = wdf.to_crs(epsg=3857)\n",
    "\n",
    "    ax = fig.gca()\n",
    "    ax = wdf.plot(\n",
    "        ax=ax,\n",
    "        column='density',\n",
    "        edgecolor='w',\n",
    "        linewidth=0,\n",
    "        cmap=parameters[var]['cmap'],\n",
    "        alpha=.6, \n",
    "        zorder=5, \n",
    "        marker='s',\n",
    "        markersize=135)\n",
    "\n",
    "    ax=GND.plot(\n",
    "        ax=ax, \n",
    "        marker=parameters[var]['mtype'],\n",
    "        lw=1,\n",
    "        # edgecolor='b',\n",
    "        facecolors=parameters[var]['mcolor'],\n",
    "        edgecolor=parameters[var]['mcolor'],\n",
    "        # color=lighten_color(parameters[var]['mcolor'], lighten_ratio), \n",
    "        markersize=150, \n",
    "        alpha=.8,\n",
    "        zorder=10)\n",
    "\n",
    "    ctx.add_basemap(ax, source=ctx.sources.ST_TONER_BACKGROUND,alpha=.5)  ###IXC\n",
    "\n",
    "\n",
    "# ======================== Time stamp ===================== START\n",
    "ax.text(\n",
    "    0.925, 0.05,\n",
    "    str(time).split()[0], \n",
    "    transform=ax.transAxes,\n",
    "    fontweight='bold',\n",
    "    fontsize=20,\n",
    "    color='w',\n",
    "    verticalalignment='bottom', \n",
    "    horizontalalignment='right', \n",
    "    bbox=dict(\n",
    "        boxstyle='round', \n",
    "        facecolor='midnightblue', \n",
    "        alpha=0.9)\n",
    ")\n",
    "# ======================== Time stamp ===================== END\n",
    "\n",
    "\n",
    "# ================== Add bars of FN, TP, FP ==============START\n",
    "width_ratio = barWidth / width\n",
    "height_ratio = barHeight / height\n",
    "parameters[events[0]]['bar_location'] = [0.625, 0.75, width_ratio, height_ratio]\n",
    "parameters[events[1]]['bar_location'] = [0.85, 0.75, width_ratio, height_ratio]\n",
    "\n",
    "plt.gcf().patches.extend([\n",
    "    plt.Rectangle(\n",
    "        (0.6, 0.7), \n",
    "        0.6, 0.25, \n",
    "        fill=True, \n",
    "        color='w', \n",
    "        alpha=0.9, \n",
    "        zorder=1,\n",
    "        transform=fig.transFigure, \n",
    "        figure=plt.gcf())\n",
    "])\n",
    "\n",
    "for var in events:\n",
    "    cmap = parameters[var]['cmap']\n",
    "    bar_location = parameters[var]['bar_location']\n",
    "    \n",
    "    bar_data = np.array([\n",
    "        parameters[var]['fn'], \n",
    "        parameters[var]['tp'], \n",
    "        parameters[var]['fp']\n",
    "    ])\n",
    "    bar_data = bar_data / bar_data.sum()\n",
    "\n",
    "    \n",
    "    ax2 = plt.gcf().add_axes(bar_location, zorder=20)\n",
    "    ax2.patch.set_alpha(1)\n",
    "    ax2.set_facecolor(\"white\")\n",
    "    \n",
    "    ax2.bar(\n",
    "        ['FN','TP','FP'],\n",
    "        bar_data,\n",
    "        color=parameters[var]['bcolor'],\n",
    "        lw=0,\n",
    "        zorder=20,\n",
    "        alpha=.9)\n",
    "\n",
    "    ax2.spines['bottom'].set_color('black')\n",
    "    ax2.spines['top'].set_color('black') \n",
    "    ax2.spines['right'].set_visible(False) \n",
    "    ax2.spines['left'].set_visible(False) \n",
    "    ax2.tick_params(axis='x', colors='black', pad=8)\n",
    "    ax2.tick_params(axis='y', colors='black')\n",
    "\n",
    "    ax2.set_title(\n",
    "        var_title_map[var], \n",
    "        color='black', \n",
    "        fontdict={'fontsize': barTitleSize, 'fontweight': 'bold'})\n",
    "    ttl = ax2.title\n",
    "    ttl.set_position([.5, 1.05])\n",
    "    ax2.grid(True)\n",
    "\n",
    "    for label in ax2.get_xticklabels():\n",
    "        label.set_color('black')\n",
    "        label.set_fontsize(barTickerSize)\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "    for label in ax2.get_yticklabels():\n",
    "        label.set_color('black')\n",
    "        label.set_fontsize(barTickerSize)\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "    ax2.tick_params(axis=u'both', which=u'both',length=0)\n",
    "# ================== Add bars of FN, TP, FP ==============END\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "sp.saveFIG(f'snapshot_{day}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
