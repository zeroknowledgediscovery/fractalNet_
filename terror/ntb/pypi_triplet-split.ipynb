{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cynet.cynet as cn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Event Log File and Generate Split and Triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. File\n",
    "In the following cell, we specify the event log input file as `LOGFILE`. Since the `STOREFILE` is only used internally, we can just name it after the `LOGFILE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGFILE = '/project2/ishanu/YI_terror/data/terror.csv'\n",
    "STOREFILE = os.path.join('/project2/ishanu/YI_terror/ntb', os.path.basename(LOGFILE).split('.')[0] + '.p')\n",
    "# In this case, STOREFILE = '/project2/ishanu/YI_terror/ntb/terror.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Spatial Range and Discretization\n",
    "\n",
    "In the following cell, we specify the tiles used for spatial discretization.\n",
    "We cut latitude (longitude) between `lat_min` and `lat_max` (`lon_min` and `lon_max`) into `lat_eps` (`lon_eps`) equal parts. Each tiles is one longitude step size wide and one latitude step size high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names in the event log file for coordinate 1 and 2\n",
    "coord1, coord2 ='latitude', 'longitude'\n",
    "\n",
    "# Tiles\n",
    "lat_min, lat_max = -4, 49\n",
    "lon_min, lon_max = -16, 84\n",
    "lat_eps, lon_eps = 50, 50\n",
    "\n",
    "lat = np.around(np.linspace(lat_min, lat_max, lat_eps + 1), decimals=5)\n",
    "lon = np.around(np.linspace(lon_min, lon_max, lon_eps + 1), decimals=5)\n",
    "tiles = [[lat[i], lat[i + 1], lon[j], lon[j + 1]] for i in np.arange(lat_eps) for j in np.arange(lon_eps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Time Range and Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names in the event log file for year, month, and day\n",
    "year, month, day='iyear', 'imonth', 'iday'\n",
    "init_date, end_date, freq = '2012-01-01', '2016-12-31', 'D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Event\n",
    "If a time series has an event frequency less than `threshold`, discard the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\n",
    "    'number_of_kills': {\n",
    "        'col_name': 'nkill',\n",
    "        'value_limits': [0, 10000],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'NKILL'\n",
    "    },\n",
    "    'BEFIA': {\n",
    "        'col_name': 'attacktype1_txt',\n",
    "        'types': [[\n",
    "            'Bombing/Explosion', \n",
    "            'Facility/Infrastructure Attack'\n",
    "        ]],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'BEFIA'\n",
    "    },\n",
    "    'AAHHH': {\n",
    "        'col_name': 'attacktype1_txt',\n",
    "        'types': [[\n",
    "            'Armed Assault', \n",
    "            'Assassination',\n",
    "            'Hijacking',\n",
    "            'Hostage Taking (Barricade Incident)',\n",
    "            'Hostage Taking (Kidnapping)'\n",
    "        ]],\n",
    "        'threshold': 0.025,\n",
    "        'csvname_prefix': 'AAHHH'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating Time Series for Training and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Time Series for the Number of kills\n",
    "Our first fit is `S0` for time series of number of kills. \n",
    "Essentially, we are looking for tiles that meet a certain number of kills (deaths in the column `nkill`). \n",
    "We are looking for tiles with number of kills that are greater than a certain `threshold`. \n",
    "Here that `threshold` is $0.025$.\n",
    "A file named `NKILL.csv` is outputted. \n",
    "And, more importantly, the internal timeseries dataframe is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = cn.spatioTemporal(\n",
    "    # File\n",
    "    log_file=LOGFILE,\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['number_of_kills']['col_name'],\n",
    "    value_limits=event_dict['number_of_kills']['value_limits'],\n",
    "    threshold=event_dict['number_of_kills']['threshold'])\n",
    "\n",
    "S0.fit(csvPREF=event_dict['number_of_kills']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we are now going to use the tiles selected for in `S0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [47:54<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "tiles = S0.getGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Time Series for Bombing/Explosion and Facility/Infrastructure Attack\n",
    "`S1` will be our fitting for attack types in the categories \n",
    " - `Bombing/Explosion` and \n",
    " - `Facility/Infrastructure Attack`.\n",
    "\n",
    "We are counting the number of these types of events that happen in these tiles.\n",
    "Output is written to `BEFIA.csv`, which contains the timeseries for those types of attacks in the selected tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [49:36<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "S1 = cn.spatioTemporal(\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['BEFIA']['col_name'],\n",
    "    types=event_dict['BEFIA']['types'],\n",
    "    threshold=event_dict['BEFIA']['threshold'])\n",
    "\n",
    "S1.fit(csvPREF=event_dict['BEFIA']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Time Series for Armed Assault, Assassination, Hijacking, and Hostage Taking\n",
    "`S2` fits for the attack types:\n",
    " - `Armed Assault`, \n",
    " - `Hostage Taking (Barricade Incident)`, \n",
    " - `Hijacking`, \n",
    " - `Assassination`,\n",
    " - `Hostage Taking (Kidnapping) `.\n",
    "\n",
    "Output is written to `AAHHH.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [46:34<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "S2 = cn.spatioTemporal(\n",
    "    log_store=STOREFILE,\n",
    "    # Spatial\n",
    "    coord1=coord1,\n",
    "    coord2=coord2,\n",
    "    grid=tiles,\n",
    "    # Temporal\n",
    "    year=year,\n",
    "    month=month,\n",
    "    day=day,\n",
    "    init_date=init_date,\n",
    "    end_date=end_date,\n",
    "    freq=freq,\n",
    "    # Event\n",
    "    EVENT=event_dict['AAHHH']['col_name'],\n",
    "    types=event_dict['AAHHH']['types'],\n",
    "    threshold=event_dict['AAHHH']['threshold'])\n",
    "\n",
    "S2.fit(csvPREF=event_dict['AAHHH']['csvname_prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Triplet for Training and Split for Testing\n",
    "Now we use the csv files created in previous steps (listed in `CSVfiles`) to generate the triplet files for training and split files for testing. \n",
    "\n",
    " - The triplet files are generated with `readTS`.\n",
    "    The training period is defined by `begin` and `end`. \n",
    " - The split files are generated with `splitTS`. \n",
    "    The split files contains data from `begin` to `extended_end`. \n",
    "    The data for testing are those beyond the `end` and before the `extended_end`\n",
    "    Here we set the `extended_end` to be one year beyond the `end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVfiles = [val['csvname_prefix'] + '.csv' for _, val in event_dict.items()]\n",
    "\n",
    "begin, end, extended_end = init_date, '2015-12-31', end_date\n",
    "\n",
    "# Make sure the triplet folder and split folder exist\n",
    "os.mkdir('/project2/ishanu/YI_terror/ntb/triplet/')\n",
    "os.mkdir('/project2/ishanu/YI_terror/ntb/split/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet\n",
    "triplet_fnames_prefix = '/project2/ishanu/YI_terror/ntb/triplet/TERROR_' + begin + '_' + end\n",
    "cn.readTS(\n",
    "    CSVfiles, \n",
    "    csvNAME=triplet_fnames_prefix, \n",
    "    BEG=begin, \n",
    "    END=end)\n",
    "\n",
    "# Split\n",
    "split_dirname = '/project2/ishanu/YI_terror/ntb/split/'\n",
    "split_prefix = begin + '_' + extended_end + '_'\n",
    "cn.splitTS(\n",
    "    CSVfiles, \n",
    "    BEG=begin, \n",
    "    END=extended_end, \n",
    "    dirname=split_dirname, \n",
    "    prefix=split_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Optional cleanup of out-of-use files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CSVfile in CSVfiles:\n",
    "    os.remove(CSVfile)\n",
    "os.remove(STOREFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation\n",
    "Now that we training and testing data ready, it is time to create the models.\n",
    "\n",
    "**Input and Output of this step**\n",
    " - Input: training data (the triplet files) produced by `readTS`;\n",
    " - Output: model json files which each represnts a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting\n",
    "\n",
    "**Note:** It is highly recommended that we use absolute paths.\n",
    "\n",
    "**Explanations:**\n",
    " - `PARTITION`: Since we work with event counts, a single partitioning at $-.5$ makes \"no event\" a $0$, and \"any number of events more than $1$\" a $1$.\n",
    " - `RUN_LOCAL`: \n",
    "     - If `False`, `xgModels` will produce a list of calls `program_calls.txt` that needs to be run to produce the models.\n",
    "     - If `True`, `xgModels` will generate models locally. \n",
    " - `NUM_RERUNS`: Since `XgenESeSS` is random, we usually run it several times to get the averaged result.\n",
    " - `XgenESeSS`: The location of the `XgenESeSS` binary.\n",
    "     - it only work for Linux;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File parameters\n",
    "TS_PATH = triplet_fnames_prefix + '.csv' # The time series (data only)\n",
    "NAME_PATH = triplet_fnames_prefix + '.coords' # The names for each time series\n",
    "FILEPATH = '/project2/ishanu/YI_terror/ntb/models/' # Make sure to create a folder with name `FILEPATH` below\n",
    "LOG_PATH = 'log.txt'\n",
    "\n",
    "# XgenESSeS parameters\n",
    "BEG = 1  # minimum delay considered\n",
    "END = 60 # maximum delay considered\n",
    "NUM_RERUNS = 2 # number of reruns\n",
    "PARTITION = [.5] # partitioning points. \n",
    "XgenESeSS = '/project2/ishanu/YI_terror/bin/XgenESeSS'\n",
    "RUN_LOCAL = False\n",
    "\n",
    "# make sure a folder named `models` is created\n",
    "os.mkdir(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running `xgModels` to generate model or model generating calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG = cn.xgModels(\n",
    "    TS_PATH,\n",
    "    NAME_PATH, \n",
    "    LOG_PATH,\n",
    "    FILEPATH, \n",
    "    BEG, \n",
    "    END, \n",
    "    NUM_RERUNS, \n",
    "    PARTITION,\n",
    "    XgenESeSS,\n",
    "    RUN_LOCAL)\n",
    "XG.run(workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Here we evaluate our models by the AUC of the their prediction. \n",
    "\n",
    "The inner working of `run_pipeline`:\n",
    "1. It first select `model_nums` number of models either by gamma or distance. \n",
    "    Then it creates a model_sel json file which is a filtered version of the models.\n",
    "1. It applies the `cynet` binary to the model_sel files, which generates a log file containing predictions.\n",
    "1. It applies the `flexroc` binary to the log files, once for each target type.\n",
    "1. Finally, it writes test statistics (AUC, fpr, and, tpr) and output a `res_all.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter setting\n",
    "\n",
    "**Explanation:**\n",
    " - `RUNLEN`: number of time steps in training and testing;\n",
    " - `FLEX_TAIL_LEN`: number of time steps in testing;\n",
    " - `model_nums`: maximum number of models to use in prediction;\n",
    " - `horizon`: prediction horizon;\n",
    " - `VARNAME`: the predicting variable types;\n",
    "    Here we use individual variable types and `ALL` meaning all types of predicting variables are used together.\n",
    " - `gamma`: If `gamma` is true, the models are sorted with gamma (coefficient of causal dependence) and the best `model_nums` models will be used in the prediction;\n",
    " - To sort models by distance, use `distance=True` instead of `gamma=True` in `run_pipeline`;\n",
    " - `cores`: Number of cores running in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File parameters\n",
    "MODEL_GLOB = '/project2/ishanu/YI_terror/ntb/models/*model.json'\n",
    "RESPATH = '/project2/ishanu/YI_terror/ntb/models/*model*res'\n",
    "DATA_PATH = os.path.join(split_dirname, split_prefix) # the split files path prefix \n",
    "\n",
    "# Prediction parameters\n",
    "RUNLEN = len(pd.date_range(start=begin, end=extended_end, freq=freq))\n",
    "FLEX_TAIL_LEN = len(pd.date_range(start=end, end=extended_end, freq=freq))\n",
    "model_nums = [20]\n",
    "horizon = 7\n",
    "VARNAME = list(set([fname.split('#')[-1] for fname in glob(DATA_PATH + \"*\")])) + ['ALL']\n",
    "\n",
    "# Running parameters\n",
    "# Make sure you have multi-core access when using cores greater than 1. \n",
    "cores = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 305 out of 305 | elapsed: 34.5min finished\n"
     ]
    }
   ],
   "source": [
    "cn.run_pipeline(\n",
    "    MODEL_GLOB,\n",
    "    model_nums, \n",
    "    horizon, \n",
    "    DATA_PATH, \n",
    "    RUNLEN, \n",
    "    VARNAME, \n",
    "    RESPATH, \n",
    "    FLEX_TAIL_LEN=FLEX_TAIL_LEN,\n",
    "    cores=cores,\n",
    "    gamma=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use see a summary of the aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    301.000000\n",
       "mean       0.789952\n",
       "std        0.057548\n",
       "min        0.318740\n",
       "25%        0.761022\n",
       "50%        0.789685\n",
       "75%        0.818124\n",
       "max        0.933031\n",
       "Name: auc, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.read_csv('res_all.csv')\n",
    "res[ (res['varsrc'] == 'ALL') & (res['auc'] < .999)]['auc'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
